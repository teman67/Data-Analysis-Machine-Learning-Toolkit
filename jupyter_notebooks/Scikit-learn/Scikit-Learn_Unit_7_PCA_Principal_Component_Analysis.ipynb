{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 07 - PCA (Principal Component Analysis)\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand what PCA (Principal Component Analysis) is and how it can be used in your project\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 06 - PCA (Principal Component Analysis)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Principal Component Analysis, or PCA, is a transformation to your data and attempts to find out what features explain the most variance in your data.\n",
    "\n",
    "* It reduces the number of variables, while it preserves as much information as possible. Therefore it is also reffered as \"dimensionality reduction\".\n",
    "* After the transformation, it creates a set of components, where each component contains the relevant information from the original variables.\n",
    "  * Each component explains a certain part of the variance of the whole dataset and is independent (uncorrelated) from each other.\n",
    "  * The drawback of PCA is that it is not easy to understand what each of these components represents since they don't relate one to one to a specific variable, instead each component corresponds to a combination of the original variabl \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **We will not focus** on the mathematical study of PCA but instead will discuss the idea behind it and how to use PCA in practical terms in your data science project\n",
    "* It will take time and experience to understand how the PCA algorithm works. For now, the central aspect is to understand what PCA is and why it will help you in predictive modelling.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> **Why and when should I consider using PCA?**\n",
    "\n",
    "\n",
    "* Imagine if your data has a lot of variables (or dimensions). \n",
    "\n",
    "  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " You want to be able to **visualize** your data to discover patterns, however it is unfeasabble to visualize all of your data in a single plot. You can use PCA to reduce your dataset to 2 or 3 components, and viualize it. We will explore that in this notebook\n",
    "\n",
    "  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    "  In **predictive modelling**, we are concerned about which variables are more relevant for modelling. PCA is a tool capable of transforming your data, retaining only the most appropriate information or the most variance while keeping all the original variables that help the model learn the patterns in the data.\n",
    "  * In supervised learning, you can use PCA as a step when extracting features for your ML model. Instead of using, for example, SelectFromModel(). You may also use PCA to transform your features into relevant components that can help to predict your target variable. We will explore this technique in the Walkthrough Project 02.\n",
    "  * In addition, in unsupervised learning, you can use PCA as a step to reduce dimensionality. So your cluster algorithm will be able to understand better how to group similar data. We will explore this technique in the next lesson and also in Walkthrough Project 02.\n",
    "\n",
    "\n",
    "You can import PCA using the command below\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "In the next cells we are going to:\n",
    "* Load a dataset and define the pipeline steps to prepare the data for PCA\n",
    "* Transform the data using PCA and understand how many components to consider\n",
    "* Visualize the data after the PCA transformation\n",
    "\n",
    "---\n",
    "\n",
    "### Load Data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's load the breast cancer data from sklearn and apply PCA\n",
    "* It shows records for a breast mass sample and a diagnosis informing whether it is as malignant or benign cancer, where 0 is malignant, 1 is benign. \n",
    "* The target variable is 'diagnostic' and the features the remaining variables.\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">  We know in advance this dataset has only numerical feaures and no missing data. \n",
    "* We are adding on purpose missing data (`np.NaN`) in the first 10 rows of 'mean smoothness' using `.iloc[:10,4]`, just to better simulate the datasets you will likely face in the workplace\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df_clf = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_clf['diagnostic'] = pd.Series(data.target)\n",
    "df_clf = df_clf.sample(frac=0.6, random_state=101)\n",
    "df_clf.iloc[:10,4] = np.NaN\n",
    "\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are interested in applying PCA to the features only (not the diagnostic)\n",
    "* We create 2 distinct DataFrames, `X` which is the features, and `df_target` that contains the diagnostic (benign or malignant). \n",
    "  * Note, there are 30 features in `X`\n",
    "  * We will use `X` to apply PCA, and `df_target` at a later stage when we visualize the data \n",
    "\n",
    "\n",
    "df_target = df_clf[['diagnostic']]\n",
    "X = df_clf.drop(['diagnostic'], axis=1)\n",
    "print(X.shape)\n",
    "X.head(3)\n",
    "\n",
    "---\n",
    "\n",
    "### Create pipeline steps\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> To apply PCA, we should scale the data. Therefore we create our pipeline that is responsible for data cleaning, feature engineering and feature scaling.\n",
    "* In our case, it will perform data cleaninig (median imputation) and feature scaling\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "### Data Cleaning\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def PipelineDataCleaningFeatEngFeatScaling():\n",
    "  pipeline_base = Pipeline([\n",
    "                            \n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median') ),\n",
    "\n",
    "      ( 'feature_scaling', StandardScaler() ),\n",
    "  ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "PipelineDataCleaningFeatEngFeatScaling()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We fit and transform the data to the pipeline.\n",
    "* The result is a NumPy array. Note there is still the same amount of rows and columns (341, 30); the point is that the data type is now an array due to the feature scaling transformation.\n",
    "\n",
    "pipeline_pca = PipelineDataCleaningFeatEngFeatScaling()\n",
    "df_pca = pipeline_pca.fit_transform(X)\n",
    "print(df_pca.shape,'\\n', type(df_pca))\n",
    "\n",
    "Just for learning purpose, let's check `df_pca`. \n",
    "* As we expect, it is the familiar NumPy array we covered in previous sections. Note also it is a 2D array.\n",
    "\n",
    "df_pca\n",
    "\n",
    "---\n",
    "\n",
    "### PCA transformation\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Now that the data is scaled, we can apply PCA component\n",
    "* We are not assemblng PCA to a pipeline in this lesson, we will do that at a later stage. The idea here is to understand how the process works\n",
    "* **A quick recap**: PCA reduces the number of variables, while it preserves as much information as possible. After the transformation, it creates a set of components, where each component contains the relevant information from the original variables.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Question%20mark%20icon.png\n",
    "\"> The first question is: \n",
    "* **How many components should I consider?** That depends; let's test, setting the number of components as the number of columns the scaled data has, in this case, 30. That is useful in understanding the explained variance of each component. \n",
    "* Read the pseudo code below to understand its logic. Once you run the cell, you will notice that:\n",
    "  * The first three components are more significant than the others. And, together, they sum 72.47% of the data variance. That is okay. It is a good sign when in a few components, like 3 or 4, you can get more than 80% of your data variance. So you could select three as the number of components, which is good progress since you had 30 features and now have three components.\n",
    "  * But in this exercise, for learning purposes, we will aim for more than 90% of data variance and use seven components since we could get more data variance with a relatively low increase of components. Since before, we had 30 features with all data variance. Then switched to 3 components with 72% of data variance, and now seven components with 90% of data variance.\n",
    "  \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA # import PCA from sklearn\n",
    "\n",
    "n_components = 30 # set the number of components as all columns in the data\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)  # set PCA object and fit to the data\n",
    "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
    "\n",
    "\n",
    "# the PCA object has .explained_variance_ratio_ attribute, which tells \n",
    "# how much information (variance) each component has \n",
    "# We store that to a DataFrame relating each component to its variance explanation\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "# prints how much of the dataset these components exaplain (naturally in this case will be 100%)\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "print(dfExplVarRatio)\n",
    "\n",
    "In the next cell we just copied the code from the cell above and changed n_components to 7. \n",
    "* With 7 components we achieved a bit more than 91% of data variance\n",
    "\n",
    "n_components = 7\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)\n",
    "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
    "\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "print(dfExplVarRatio)\n",
    "\n",
    "Note that the data is transformed and stored at `x_PCA`. Let's check its content\n",
    "* You will notice it is a NumPy array, and its dimension is 341 x 7, where the rows indicate the number of rows and seven relates to the number of components we defined earlier\n",
    "* Imagine now that this data would be fed to a model. For this particular dataset, the ML task would be a classification.\n",
    "* Also note that the PCA helped reduce from 30 features to 7 components where these seven components contain 90% of the information.\n",
    "\n",
    "print(x_PCA.shape)\n",
    "x_PCA\n",
    "\n",
    "---\n",
    "\n",
    "### Visualize data after PCA transformation\n",
    "\n",
    "Imagine you want to visualize your data, before and after applying PCA.\n",
    "* If you had to visualize the 30 features, you could do a correlation analysis and look for features that are correlated among themselves or, in this particular dataset, features that are correlated to the target.\n",
    "* So let's suppose you want to visualize the relationship between \"mean concavity\" and \"mean concave points\" and the target. Since the features are numerical, you can do a scatter plot with them and color by the target.\n",
    " * You will imagine/visualize the frontier between the blue and orange dots. Although that is good, the malignant and benign may look to be separable. At the same time, few data points look mingled in this frontier.\n",
    "  * However, what about the remaining variables? When you consider this dataset as a whole, is that informative enough to separate these classes?\n",
    "\n",
    "var1, var2 = 'mean concavity' , 'mean concave points'\n",
    "sns.scatterplot(x=X[var1], y=X[var2], hue=df_target['diagnostic'])\n",
    "plt.xlabel(var1)\n",
    "plt.ylabel(var2)\n",
    "plt.show()\n",
    "\n",
    "We can plot the PCA components to evalute, from another perspective, how the data behaves.\n",
    "* We know x_PCA holds the data after transformation, and has 7 components. We will plot in a scatterplot the most representative components: components 0 and 1\n",
    "\n",
    "sns.scatterplot(x=x_PCA[:,0], y=x_PCA[:,1])\n",
    "plt.xlabel('Component 0')\n",
    "plt.ylabel('Component 1')\n",
    "plt.show()\n",
    "\n",
    "We know that these 2 components hold by themselves 62% of the information (data variance).\n",
    "* This is powerful because with 2 variables (2 components) we have a clearer vision on how the dataset looks to have enough information to separate malignant and benign\n",
    "* We now color the plot by diagnostic using df_target as the hue argument.\n",
    "  * Note we see a clearer border between 0 and 1.\n",
    "  * In a nutshell, we have the same data, showing the same information. The difference now is that the data was reduced to its major components\n",
    "  * The drawback is that we lose the interpretation, since component 0 is made of a combination of the original variables\n",
    "\n",
    "sns.scatterplot(x=x_PCA[:,0], y=x_PCA[:,1], hue=df_target['diagnostic'], alpha=0.8)\n",
    "plt.xlabel('Component 0')\n",
    "plt.ylabel('Component 1')\n",
    "plt.show()\n",
    "\n",
    "Naturally We can plot more components. In this exercise we can plot 3 components in a 3D scatter plot using Plotly Express\n",
    "  * Move around the 3D plot and try to visualize if you could draw a surface that would separate the dots. The surface you imagined, is a ML model.\n",
    "  * Note again these 3 components alone hold 72% of all information from the dataset to diagnose malignant or benign.\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(x=x_PCA[:,0], y=x_PCA[:,1], z= x_PCA[:,2] , color=df_target['diagnostic'],\n",
    "                    labels=dict(x=\"Component 0\", y=\"Component 1\", z='Component 2'),\n",
    "                    color_continuous_scale='spectral',\n",
    "                    width=750, height=500)\n",
    "fig.update_traces(marker_size=5)\n",
    "fig.show()\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
