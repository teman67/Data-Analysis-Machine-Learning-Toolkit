{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 02 - Split your data, fit a model, predict and save the model\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and implement the basic workflow for splitting the data, fitting a model, predicting on data and saving the model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 02 - Split your data, fit a model and predict\n",
    "\n",
    "In this unit, we will cover how to:\n",
    "  * Split your data\n",
    "  * Fit a model\n",
    "  * Run predictions with the fitted model\n",
    "  * Save the model, so you can use it later\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In supervised learning, you are interested in splitting your data. In conventional ML, like in Scikit-learn, we will split the data into Train and Test sets.\n",
    "* The validation set is a part of the Train set. When using a specific Scikit-learn function for hyperparameter optimization, the validation set is grabbed automatically. Therefore we will split into Train and Test set only.\n",
    "* If you want a refresher on Train, Validation, Test set, revert to Module 2 - ML Essentials.\n",
    "\n",
    "Let's consider the iris dataset. It contains records of 3 classes of iris plants, with its petal and sepal measurements\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
    "df = pd.read_csv(url)\n",
    "# df = sns.load_dataset('iris')\n",
    "df.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> How do you know which variables are features and which variable is a target variable?\n",
    "* It will depend on the context of your ML project. You will need to know or need to investigate the objective of your ML project to determine features and the target.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For this dataset, species is the target variable. There are three species. We need to classify the species according to the flower's petal and sepal. Our ML task then will be a classification.\n",
    "\n",
    "df['species'].value_counts()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Split your data\n",
    "\n",
    "We use `train_test_split()` to split the data. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). The parameters we will use are:\n",
    "* The first 2 are the features and target, respectively. In this case, for the features, you drop species, and for the target, you subset species.\n",
    "* ``test_size:`` it represents the data proportion to include in the test set. We set at 0.2\n",
    "* ``random_state:`` according to the documentation, it controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. It can be any positive integer. We suggest keeping the same random_state value across your project. We will select here 101\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> `random_state` is a critical parameter in ML, which we will use in other use cases. It essentially gives **REPRODUCIBILITY** to your project. That means the same result you get here right now, and another person will get somewhere else at another time.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['species'],axis=1),\n",
    "                                                    df['species'],\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=101)\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "Let's have look at `X_train`\n",
    "* Those will be the features used to train the model\n",
    "* Note the features are numbers. Scikit-learn uses numbers to fit models. That is why we have to encode categorical data\n",
    "* In this dataset, we don't need any data cleaning or categorical encoding\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "Let's inspect `y_train`. These are categories.\n",
    "* When the ML task is classification, Sckit learn handles either number or categories for the target variable\n",
    "\n",
    "y_train\n",
    "\n",
    "In addition, `y_train` is a Pandas Series\n",
    "\n",
    "type(y_train)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Fit your model\n",
    "\n",
    "You get a preview of tree-based algorithms in Module 2. Even though we have a dedicated unit for tree-based algorithms, we will use a decision tree algorithm to fit a model to give a sense of the basic workflow for fitting a model.\n",
    "* We will use `DecisionTreeClassifier()`, the documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "* We create a python object/variable called model, and instantiate `DecisionTreeClassifier()`. A common convention is to set the object name as a model.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">  Note: we created a model and fit. We can do that since the data doesn't require a pre-processing step, like data cleaning or categorical encoding, for the purpose of fitting.\n",
    "\n",
    "* Fitting only the model is fine as a learning experience. However, in our exercises, we will not fit the model but instead use a pipeline that contains a series of steps, where typically, the last step will be the model.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "Next, we fit the model with the train set - features (`X_train`) and target (`y_train`)\n",
    "* We use `.fit()` method and parse `X_train` and `y_train`. Simple as that.\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Run predictions\n",
    "\n",
    "Let's predict the test set using our model.\n",
    "* We use `.predict()` and parse the test set features (`X_test`)\n",
    "* The answer is an array\n",
    "\n",
    "model.predict(X_test)\n",
    "\n",
    "You can predict the probability (between 0.0 and 1.0) for each class for a given observation using `.predict_proba()`\n",
    "\n",
    "model.predict_proba(X_test)\n",
    "\n",
    "Ideally, we should predict on the Train and Test set, set a performance metric and evaluate model performance.\n",
    "* We will not evaluate the model yet. We will leave it until another unit\n",
    "* The idea here is to feel how it works behind the hood to do a basic training and predicting process.\n",
    "\n",
    "Let's assume now you want to predict on real-time data.\n",
    "* In an application, you will likely create an interface to collect the data or will get the data from somewhere else, from an API, for example.\n",
    "* In this case, we will manually create a DataFrame that contains the features. We call that X_live. It will have one row only (you could have a set of rows, that would mean to run predictions in a batch, in our case, it is only one prediction)\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In theory, you can set any value to the variable, But in practice, the values will follow the actual data distribution.\n",
    "\n",
    "X_live = pd.DataFrame(data={'sepal_length':6.0,\n",
    "                            'sepal_width':3.9,\n",
    "                            'petal_length':2.5,\n",
    "                            'petal_width':0.9},\n",
    "                      index=[0] # the DataFrame needs an index (either number or category), we just parsed the number 0\n",
    "                      )\n",
    "X_live\n",
    "\n",
    "Let's predict on the live data\n",
    "\n",
    "model.predict(X_live)\n",
    "\n",
    "The model is 100% confident it is a determined class\n",
    "\n",
    "model.predict_proba(X_live)\n",
    "\n",
    "We noticed already this class is Versicolor, but you cross-check the labels orders with .unique()\n",
    "\n",
    "df['species'].unique()\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Save your model\n",
    "\n",
    "We can save either an ML model or an ML pipeline as a .pkl file with a library called joblib.\n",
    "* You need the function `joblib.dump()`, the documentation is [here](https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html). We will parse the arguments `value`, as the file we want to save, and `filename` as the directory + filname + .pkl (we are saving at root) \n",
    "\n",
    "import joblib\n",
    "joblib.dump(value=model , filename=\"my_first_ml_model.pkl\")\n",
    "\n",
    "Once you are in an application or in another notebook, you can load with `joblib.load()`. The documentation is [here](https://joblib.readthedocs.io/en/latest/generated/joblib.load.html). You will parse the argument `filename` as the directory + filename + .pkl\n",
    "\n",
    "loaded_model = joblib.load(filename=\"my_first_ml_model.pkl\")\n",
    "loaded_model\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Awesome!! \n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Does that mean I am ready to create an ML model for the world and solve big challenges?\n",
    "* Almost. We just started the ML journey now! \n",
    "* We still need to cover more topics. Let's have some fun now!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
