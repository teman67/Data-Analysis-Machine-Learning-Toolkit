{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 05A - Cross Validation Search (GridSearchCV) and Hyperparameter Optimization Regression - Part 01\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and use GridSearchCV for Hyperparameter Optimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 05A - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimization\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Good job! You fitted multiple pipelines considering different algorithms separately for regression and classification tasks. However, how do you know which was better for a given ML task? \n",
    "* Imagine for the classification task on the iris dataset, you fitted three individual pipelines, evaluated each separately, and concluded a given algorithm was better. That is fair enough, but you want a more effective way to assess multiple algorithms.\n",
    "* However, you also fitted the models with the default hyperparameters, and may wonder: what if for a given algorithm, I could fit multiple models using different hyperparameters and find a model with even better performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%208-%20Challenge.png\"> Let's learn how to use GridSearchCV and do Hyperparameter Optimization using multiple algorithms. \n",
    "* This is the heart of conventional ML. We will split this topic into 2 parts: \n",
    "  * in the next 3 notebooks, we will show how to conduct hyperparameter optimization using 1 algorithm (for Regression, Binary Classification and Multiclass Classification) \n",
    "  * Then we will cover how to do hyperparameter optimization using multiple algorithms at once.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimization with 1 algorithm\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In this section, we will select a given algorithm and fine-tune it, by defining a set of hyperparameters. \n",
    "* For each possible hyperparameter combination, a set of models will be fitted - based on the cross-validation parameter. For example, if the developer sets cross-validation as 5, it will fit 5 models for a given hyperparameter combination. \n",
    "* These 5 models are scored against a performance metric (ie.: if it is regression it could be the R2 score), and average performance is computed. This average is the cross-validated performance for a given configuration of hyperparameters. \n",
    "* This process is repeated then for every combination of hyperparameters. \n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">  Let's quickly recap how we use the data for fitting supervised models\n",
    "* The training subset of the data is used to fit or train the model.\n",
    "* A subset of the training set is known as the validation set. This is used during fitting to compare one model against another, and when choosing or tuning hyperparameters. \n",
    "* The final subset of data used to test the model is known as the test set. This assesses the final models' performance and it must be data that is new to the model to give an unbiased result. The test set closely replicates what the deployed model will see in real-time usage. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\">   When we do Hyperparameter Optimization, a part of the train set is automatically subset as a validation set and the model is fitted using cross-validation. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> After all, how can I do that in Sckit-learn? \n",
    "* We use a function called **GridSearchCV**, where it fits multiple models looping through a hyperparameter list over the model. \n",
    "* Note: CV here means cross-validation. It uses cross-validation to  compare different algorithm and hyperparameter combinations. So, in the end, we can select the best parameters from the listed hyperparameters that achieves a better performance.\n",
    "* In the end, it helps to automate the process to find the best combination of hyperparameters for a given algorithm in a given dataset. Its documentation is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will split this section showing uses cased for GridSearchCV on Regression, Binary Classification, Multiclass classification task\n",
    "* When using GridSearchCV, the difference among these ML tasks relies on the scoring parameters, which tells which performance metric should be used to select the best model.\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Regression\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " We are going to consider a similar workflow we studied earlier:\n",
    "* Split the data\n",
    "* Define the pipeline \n",
    "* Fit the pipeline\n",
    "* Evaluate the pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">The differences now are:\n",
    "* we decide on a list of hyperparameters to optimize our model while fitting it\n",
    "* we need a performance metric to decide which model is the best in cross-validating the models.\n",
    "\n",
    "\n",
    "We will use the Boston dataset from sklearn. It has house price records and house characteristics, like the average number of rooms per dwelling and Boston's per capita crime rate.\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df_reg = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_reg['price'] = pd.Series(data.target)\n",
    "\n",
    "df_reg = df_reg.sample(frac=0.5, random_state=101)\n",
    "\n",
    "print(df_reg.shape)\n",
    "df_reg.head()\n",
    "\n",
    "We split the data into train and test sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Similarly to previous examples, we set the pipeline with 3 steps, feature scaling, feature selection and modelling. \n",
    "* For the learning purpose, we just set the algorithm used as RandomForestRegressor. However, we encourage you to try by yourself additional algorithms in your free time.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def pipeline_adaboost_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestRegressor(random_state=101)) ),\n",
    "      ( \"model\", RandomForestRegressor(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We should parse the **algorithm's hyperparameters**, in a dictionary, with the support from its documentation, which is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "* Since we are fitting a pipeline, that contains a set of steps, you should state in your hyperparameter list to which step your hyperparameter belongs. In our case, we named the modelling step as `\"model\"`, so we add the suffix `\"model__\" `before the hyperparameter name.\n",
    "* For learning purposes, we picked 1 hyperparameter: n_estimators. For n_estimator, we parse in a list with 10 and 20 (the default value is 50, but for learning purpose we set 10 and 20 for a faster computation)\n",
    "* It will take time and practical experience to make sense of which hyperparameters are more useful for each algorithm and what are the typical ranges to consider when listing hyperparameter's values\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html  # documentation is here\n",
    "param_grid = {\"model__n_estimators\":[10,20],\n",
    "              }\n",
    "\n",
    "param_grid\n",
    "\n",
    "We import GridSearchCV. Its documentation is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). We parse\n",
    "*  `estimator` as the pipeline, `param_grid` as the dictionary we stated above. \n",
    "* `cv` sets the number of cross-validation you have to try for each selected set of hyperparameters. It uses k-fold cross-validation, where we subdivide the whole dataset into multiple randomly chosen data sets known as k-fold cross-validation where k refers to the number of data sets.  \n",
    "* `n_jobs`, according to the documentation, is the number of jobs to run in parallel. -1 means using all processors, where -2 uses all but one.\n",
    "* `scoring`, is the evaluation metric that you want to use.  That will depend on the ML task you are considering. In this case, it is regression, so we set the R2 score to be the metric. Other options would include: `'neg_mean_absolute_error'`, `'neg_mean_squared_error'`\n",
    "* `verbose`, according to the documentation, controls the verbosity: the higher, the more messages. For learning purposes we set as 3, so you can understand the process.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> We create an object (we simply called the grid) that contains a GridSearchCV using the parameters above. Next, we fit this object to the train set (features and target).\n",
    "\n",
    "* That will fit multiple RandomForestRegressor models. Considering cv=2 and the hyperparameters we listed above, it will train 2 models. It trains 2 models since we have 2 possible combinations of hyperparameters - \n",
    "* For each model, we use a 2-fold cross-validation, since cv=2. Therefore each model will be fit twice.\n",
    "* In total, this operation will fit 4 models, 2 models where each model is fitted 2 times, due to 2-fold cross-validation.\n",
    "* Note, the 2 scores for `model__n_estimators=10` (the first 2 lines) are 0.618 and 0.695. The average is 0.656. We will highlight to you this average when computing the cross-validation results for this hyperparameter combination in upcoming cells\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Remember, the validation set is automatically defined using GridSearchCV. You parse the training set and it will subset the validation set as a part of the training set.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=pipeline_adaboost_reg(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=2,\n",
    "                    n_jobs=-2,\n",
    "                    verbose=3,  # for learning, we set 3 to print the score from every cross validation\n",
    "                    scoring='r2')\n",
    "\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "The results of all models and their respective cross-validations are stored in the attribute `.cv_results`\n",
    "* When you access this attribute, you will notice it is a dictionary and when displayed as it is, is not very informative\n",
    "\n",
    "grid.cv_results_\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> One way to make it more informative is to parse it to a DataFrame, sort the values by 'mean_test_score', filter 'parameters' and 'mean_test_score' columns and convert to an array, using .values\n",
    "* In the end, you print a simplified ordered list showing the results for optimizing a model with multiple hyperparameters combinations.\n",
    "  * For example, we see the best hyperparameter configuration is `n_estimators 10`, which gave an R2 score of 0.65\n",
    "  * Note the first hyperparameter combination: `model__n_estimators=10`. Previously we commented on its average performance: 0.656. That is a result of the average of the 2 cross-validated models for this hyperparameter combination. In this particular example considering this set of hyperparameter combinations, its performance was the best compared to the other.\n",
    "\n",
    "(pd.DataFrame(grid.cv_results_)\n",
    ".sort_values(by='mean_test_score',ascending=False)\n",
    ".filter(['params','mean_test_score'])\n",
    ".values\n",
    " )\n",
    "\n",
    "At the same time, we can get the best parameters with `.best_params_`\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "This is interesting, but we want to have the pipeline which gave the\n",
    "highest score, so we can use it in real life. To grab it, use the attribute `.best_estimator_`\n",
    "* This is the most important aspect of the grid search, where we grab the pipeline, we will evaluate and potentially use\n",
    "* Note, when fitting the pipelines, you saw the score for each cross-validated model for each hyperparameter combination. For the best hyperparameter combination, it takes the best cross-validated model, in this case, it takes the last.\n",
    "  * `[CV 1/2] END ............model__n_estimators=10;, score=0.618 total time=   0.2s`\n",
    "  * `[CV 2/2] END ............model__n_estimators=10;, score=0.695 total time=   0.2s`\n",
    "\n",
    "pipeline = grid.best_estimator_\n",
    "pipeline\n",
    "\n",
    "You can now evaluate the pipeline that you fit using hyperparameter optimization using the techniques we covered already. We will import the custom function for regression evaluation\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "import numpy as np\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test,pipeline):\n",
    "\tprint(\"Model Evaluation \\n\")\n",
    "\tprint(\"* Train Set\")\n",
    "\tregression_evaluation(X_train,y_train,pipeline)\n",
    "\tprint(\"* Test Set\")\n",
    "\tregression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X,y,pipeline):\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test,pipeline, alpha_scatter=0.5):\n",
    "  pred_train = pipeline.predict(X_train)\n",
    "  pred_test = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test , y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Next, we parse the train set, test set and our pipeline to the function\n",
    "* Note the performance on the train test is nice (0.93). The test set is not so nice (0.68); however, these values are distant, which may indicate overfitting.\n",
    "* We may have to consider additional values for the hyperparameters or even consider other hyperparameters. Or maybe we need more data so the algorithm can find the patterns and generalize on unseen data.\n",
    "* Or maybe this algorithm is not the best for this dataset. But don't worry, soon we will discover an approach to train multiple algorithms at once.\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
