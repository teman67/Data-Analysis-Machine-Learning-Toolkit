{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 05C - Cross Validation Search (GridSearchCV) and Hyperparameter Optimization Multiple Clf- Part 01\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and use GridSearchCV for Hyperparameter Optimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 05C - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimization\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimization with 1 algorithm\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Multiclass Classification\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In the last section, we saw how to conduct a hyperparameter tuning using one algorithm to solve a Binary Classification problem.\n",
    "* There is a tiny difference for using GridSearch CV when your ML task is multi-class classification, we will cover that now.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are going to consider a similar workflow we studied earlier:\n",
    "* Split the data\n",
    "* Define the pipeline and hyperparameters\n",
    "* Fit the pipeline\n",
    "* Evaluate the pipeline\n",
    "\n",
    "We load the iris dataset for this exercise. It contains records of 3 classes of iris plants, with its petal and sepal measurements\n",
    "\n",
    "df_clf = sns.load_dataset('iris')\n",
    "\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "As usual, we split the data into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "And create a pipeline using 3 steps: feature scaling, feature selection and modelling with RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def pipeline_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestClassifier(random_state=101)) ),\n",
    "      ( \"model\", RandomForestClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We define our hyperparameter list based on the algorithm documentation.\n",
    "* In this case, there will be 2 hyperparameters combinations\n",
    "* We are interested to provide you a feeling experience in hyperparmeter optimization, therefore we will reduce the amount of hyperparameter combinations, so the learning process can be faster in this moment. However, we encourage you to try by yourself additional combinations in your free time.\n",
    "\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"model__n_estimators\":[10,20],\n",
    "              }\n",
    "param_grid\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  Let's assume for this project, the client is interested in the Virginica species, and needs the predictions for this class to be precise (for some client's reason, if the model says a given flower is Virginica, it has to be Virginica, it can't be something else). \n",
    "* In this case, your scoring parameter is `precision_score` to the class Virginica\n",
    "  * In a Multiclass classification, when your performance metric is accuracy, you just parse scoring='accuracy', like in a binary classifier.\n",
    "  * In our case, we need make_scorer() to parse we want to fine-tune the model using precision on the Virginica species. We parse to make_scorer the metric we want - precision_score. The next argument is `labels`, where you parse the class you want to tune a list. Note, in this dataset, the species is not encoded as numbers but as categories. If it were numbers, you would parse the number related to the class you want to tune. The last argument is average, and it should parse None, since you compute the precision from one class only (in this case Virginica) and you don't need to average.\n",
    "* Finally, you fit the grid search to the training data.\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "grid = GridSearchCV(estimator=pipeline_clf(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=2,\n",
    "                    n_jobs=-2,\n",
    "                    verbose=3, # in the workplace we typically set verbose to 1, \n",
    "                    # to reduce the amount of messages when fitting the models\n",
    "                    # for teaching purpose, we set to 3 to see the score for each cross validated model\n",
    "                    scoring=make_scorer(precision_score,\n",
    "                                        labels=['virginica'],\n",
    "                                        average=None)\n",
    "                    )\n",
    "\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "Next, we check the results for all 4 different models with `.cv_results_` and use the same code from the previous section\n",
    "* Note this combination `''model__n_estimators': 10` gave a average precision score on virginica of 0.91. In this case, both options look to give same performance, and the grid search picked the model with n_estimator as 10\n",
    "\n",
    "(pd.DataFrame(grid.cv_results_)\n",
    ".sort_values(by='mean_test_score',ascending=False)\n",
    ".filter(['params','mean_test_score'])\n",
    ".values\n",
    " )\n",
    "\n",
    "We grab programmatically the best hyperparameter combination for a quick check\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "And finally grab the best pipeline, considering the best cross-validated model for the best hyperparameter combination\n",
    "\n",
    "pipeline = grid.best_estimator_\n",
    "pipeline\n",
    "\n",
    "We finally evaluate the pipeline\n",
    "* Note the precision on Virginica, on the train set is 98% and on the test set is 100%. It is a very good sign that the precision is maxed for the test set since it shows the pipeline can generalize on unseen data\n",
    "* Again, the client will accept the pipeline based on the performance criteria you both set in the ML business case.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "    \n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique()\n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Congratulations! You now know how to get a given algorithm and do a hyperparameter optimization for Regression and Classification!\n",
    "  * The **next level** is to define a set of algorithms and a set of hyperparameters for each algorithm, and do a hyperparameter optimization for Regression and Classification tasks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
