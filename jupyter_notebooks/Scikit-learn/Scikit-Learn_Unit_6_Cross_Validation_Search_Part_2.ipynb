{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 06 - Cross Validation Search (GridSearchCV) and Hyperparameter Optimization - Part 02\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Do Hyperparameter Optimization using multiple algorithms \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 06 - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimization\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Good job! You fitted multiple pipelines using a single algorithm while looking for the best hyperparameter combination, for regression and classification tasks. However how do you know which was better for a given ML task? \n",
    "* Let's learn how to use GridSearchCV and do Hyperparameter Optimization using **multiple algorithms**\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " We will cover in the notebook:\n",
    "* A technique to do a hyperparameter Optimization with multiple algorithms\n",
    "* Strategy for using this technique that tpyically reduces the time needed to train all algorithms\n",
    "* Strategy to refit the pipeline only with the most relevant features, so you can deploy a pipeline that contains only the best features.\n",
    "* **BONUS**: we list here the most common hyperparameters values for the algorithms we covered in the course. So you can use as a starting point and as a reference in case you need them for the Milestone Project or in the workplace.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimization with multiple algorithms\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We are going to consider a similar workflow we studied earlier:\n",
    "\n",
    "* Split the data\n",
    "* Define the pipeline and hyperparameter\n",
    "* Fit the pipeline (using a strategy that tpyically trains all the algorithms faster)\n",
    "* Evaluate the pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> The exercise we are conducting in this notebook is for a multiclassification task, extending to regression and binary tasks. The concepts we cover here are also applied to these other tasks\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will use the penguins dataset for this exercise. It has records for 3 different species of penguins, collected from 3 islands in the Palmer Archipelago, Antarctica. \n",
    "* Here, we are interested to predict the species for a given penguin\n",
    "\n",
    "df_clf = sns.load_dataset('penguins')\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "We split the data into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " And define the pipeline steps considering:\n",
    "* data cleaning (median imputation, categorical imputation)\n",
    "* feature engineering (categorical encoding)\n",
    "* feature scaling, \n",
    "* feature selection (note we don't specify the algorithm, we parse a variable called `model`)\n",
    "*  and modelling (note we dont specify the algorithm, we parse a variable called `model`)\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> Previously we covered the definition of the data cleaning and featuring steps, and for this exercise, we provide the actions (imputations and encoding).\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Data Cleaning and Feature Engineering\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "  pipeline_base = Pipeline([\n",
    "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
    "                                     variables=['bill_length_mm' , 'bill_depth_mm',\n",
    "                                                'flipper_length_mm', 'body_mass_g']) ),\n",
    "\n",
    "      ( 'categorical_imputer', CategoricalImputer(imputation_method='frequent',\n",
    "                                                        variables=['sex']) ),\n",
    "\n",
    "      ( \"ordinal\",OrdinalEncoder(encoding_method='arbitrary', \n",
    "                                 variables = ['island',\t'sex']) ), \n",
    "\n",
    "      (\"feat_scaling\", StandardScaler() ),\n",
    "\n",
    "      (\"feat_selection\",  SelectFromModel(model) ),\n",
    "\n",
    "      (\"model\", model ),\n",
    "\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next we create a Python class (HyperparameterOptimizationSearch) which aims to fit a set of algorithms with multiple hyperparameters.The logic is: \n",
    "* The developer defines a set of algorithms and their respectives hyperparameters values\n",
    "* The code iterates on each algoirthm and fits pipelines using GridSearchCV considering its respective hyperparameter values. The result is stored. \n",
    "* That is repeated for all algorithms that the user listed.\n",
    "* Once all pipelines are trained, the developercan retrieve a list with a performance result summary and an object that contains all trained pipelines. The developer can then subset the best pipeline.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> Let's explain the major parts from the Python class\n",
    "\n",
    "* the __init__ method, you parse models, params as a dictionary of algorithms and their respective hyperparameters\n",
    "* the `fit` method, we loop on each algorithm, and parse the algorithm to PipelineOptimization(). As a result, it will do a grid search on a set of hyperparameters for that given model. The result is stored and the loops goes on\n",
    "* the `score_summary` method returns all pipelines, and a DataFrame with a performance summary for all of the algorithms.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> Again, at first, it will take some time to understand the code of this class, but what's most important for now is to understand what it does. \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model = PipelineOptimization(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\">\n",
    " We now define a list of models and their respective hyperparameters. \n",
    "* The first dictionary is related to the algorithms\n",
    "  * We create a dictionary where the key is the model name (you can parse anything here, but the suggestion is to use the estimator name), and the value is the estimator object, for example, for decision tree we use DecisionTreeClassifier(random_state=0).\n",
    "* It is a multiclass classfication, so we consider all algorithms but logistic regression (since that is more suitable for binary classification)\n",
    "\n",
    "models_search = {\n",
    "    \"DecisionTreeClassifier\":DecisionTreeClassifier(random_state=0),\n",
    "    \"RandomForestClassifier\":RandomForestClassifier(random_state=0),\n",
    "    \"GradientBoostingClassifier\":GradientBoostingClassifier(random_state=0),\n",
    "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=0),\n",
    "    \"AdaBoostClassifier\":AdaBoostClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> The other dictionary relates to the hyperparameter values\n",
    "  * Its keys should map the keys from the models' dictionary.\n",
    "  * for each key, the value will be a dictionary, whose keys will be the hyperparameter names, and their values a list of hyperparameter values\n",
    "  * Look at the example, RandomForestClassifier has 2 hyperparameters: n_estimators and max_depth. For each hyperparameter we set a list with the determined values\n",
    "  * When you want to consider only the default hyperparameters, you just parse an empty dictionary for a given algorithm. You will see that for the other algorithms, they have a `{ }` as their hyperparameters, that means it will only consider the default hyperparameters. **But you may ask: why would we do that?**\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeClassifier\":{},\n",
    "    \"RandomForestClassifier\":{\"model__n_estimators\":[50,20],\n",
    "                               \"model__max_depth\":[None,3,10]},\n",
    "    \"GradientBoostingClassifier\":{},\n",
    "    \"ExtraTreesClassifier\":{},\n",
    "    \"AdaBoostClassifier\":{},\n",
    "}\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\"> That is useful when we define a strategy to speed up the hyperparameter optimization process\n",
    "* You noticed the idea is to fit multiple models with multiple hyperparameters options. But the time needed to compute all of that based on your hardware capability has a cost. \n",
    "* It would make sense to do a quick search using the default hyperparameters across all listed algorithms. The result will show the algorithms that look to fit your data the best, and this training process tends not to take long since it uses the default hyperparameters.\n",
    "* Then you use the best 2 or 3 algorithms and finally do an extensive search so that you can fine-tune your pipeline performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's do a hyperparameter optimization search using the **default hyperparameters values first**\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeClassifier\":{},\n",
    "    \"RandomForestClassifier\":{},\n",
    "    \"GradientBoostingClassifier\":{},\n",
    "    \"ExtraTreesClassifier\":{},\n",
    "    \"AdaBoostClassifier\":{},\n",
    "}\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We now use our custom class `HyperparameterOptimizationSearch` to assign a object called search (you can use the name you want)\n",
    "* We parse 2 arguments: models and params, which are the 2 dictionaries we set in the previous cells: models_search and params_search.\n",
    "* The goal here is to use the default hyperparameters to find the type of algorithms that look to best fit your data\n",
    "* Next, we fit this object, meaning we will fit all the algorithms using GridSearchCV. Therefore we parse the `training data` (X_train, y_train), `scoring` (in this case we defined accuracy, for learning purpose only) and `cv` (we defined 2 to speed up the process)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Note you will see the code looping on each algorithm. There is 1 candidate since you are fitting with the default hyperparameter. It totals 2 fits per model since cv=2\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Running GridSearchCV for DecisionTreeClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for RandomForestClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for GradientBoostingClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for ExtraTreesClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "\n",
    "Running GridSearchCV for AdaBoostClassifier \n",
    "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-1, # use all processors, but one\n",
    "           cv=2)\n",
    "\n",
    "Our method .score_summary returns a dataframe with all the training results summary and a dictionary containing all pipelines\n",
    "* We grab both and first check the results summary\n",
    "* Note that ExtraTreesClassifier had an average accuracy performance (using 2 cross validated models with default hyperparameters values) of 0.98\n",
    "* The second best was RandomForestClassifier with 0.95. Then GradientBoostingClassifier with 0.92.\n",
    "* AdaBoostClassifier had the lowest performance here, with 0.82 of average accuracy. \n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    "**In which algorithms would you spend time doing an extensive hyperparameter search?**\n",
    "* It depends on how distant is the performance among the top performers.\n",
    "* In our case, we would certainly select ExtraTreesClassifier and would give a second chance to RandomForestClassifier, since its performance was not so far from ExtraTress.\n",
    "* We wouldn't give a second chance to GradientBoosting since 0.92 (for this context) is quite far from 0.98\n",
    "\n",
    "\n",
    "* However, there could be a case where for example, the top 4 had really similar performance on the default hyperparameter, then you would do an extensive hyperparameter optimization on these four.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " Let's define the new hyperparameters for the extensive search\n",
    "* You don't need to parse the same amount of hyperparameters for each algorithm, and the assigned values in the list will depend on the hyperparameter.\n",
    "* There is no fixed number of values to be parsed in this list; just remember the more values and more hyperparameters you parse, the more time it will take to fit all possible combinations.\n",
    "\n",
    "# you don't have to necessarily list in any specific order here\n",
    "models_search = {\n",
    "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=0),\n",
    "    \"RandomForestClassifier\":RandomForestClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "    \"ExtraTreesClassifier\":{\"model__n_estimators\": [20,50],\n",
    "                            },\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    \"RandomForestClassifier\":{\"model__n_estimators\": [40,20],\n",
    "                            },\n",
    "}\n",
    "\n",
    "Let's fit again using our HyperparameterOptimizationSearch class and our updated information on models_search and params_search.\n",
    "* The other arguments remain the same\n",
    "* The goal here is to do an extensive search on the algorithms that performed better in a default hyperparameter optimization\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-1,\n",
    "           cv=2)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\">\n",
    " Let's check the results summary with .score_summary\n",
    "* We could do a further round of extensive search with more hyperparameters and consider values around those that demonstrated good performance in this round. But for teaching purposes, we will be happy with the current search.\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "Programatically, we grab the best model name, by using `.iloc[ ]` on the first row and column from the previous data frame\n",
    "\n",
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model\n",
    "\n",
    "Let's get the best model parameters\n",
    "\n",
    "grid_search_pipelines[best_model].best_params_\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Finally, we want to grab the best pipeline\n",
    "* The object grid_search_pipelines contains all trained pipelines. We first subset the pipelines from the algorithm having the best performance (with `best_model`), then use `.best_estimator_` to retrieve the pipeline that has the algorithm and hyperparameter configuration that best suits our data.\n",
    "\n",
    "best_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The best pipeline is a tree based algorithm, so we can check the most important features with `.feature_importances_`\n",
    "\n",
    "* The information on the `“best features”` are on the pipeline’s “feature selection” step as a boolean list. We get this list to subset the train set columns\n",
    "* Go through the pseudo code to understand the logic\n",
    "* Make sure you understand the variable data_cleaning_feat_eng_steps. In case you use this code in your milestone project, you will likely need to update this value to your pipeline accordingly.\n",
    "\n",
    "# after data cleaning and feature engineering, the feature space may change\n",
    "# for example, you may drop variables, or you may add variables; imagine if you have a \"date\" variable\n",
    "# and you extract day, month and year for exmaple.\n",
    "# then you ask yourself: how many data cleaning and feature engineering steps does your pipeline have?\n",
    "# in our case 3: median, categorical_imputer and ordinal\n",
    "\n",
    "data_cleaning_feat_eng_steps = 3\n",
    "# we get these steps with .steps[] starting from 0 until the value we assigned above\n",
    "# then we .transform() to the train set and extract the columns\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "# we get the boolean list indicating the best features with best_pipeline['feat_selection'].get_support()\n",
    "# and use this list to sbuset columns_after_data_cleaning_feat_eng\n",
    "best_features = columns_after_data_cleaning_feat_eng[best_pipeline['feat_selection'].get_support()].to_list()\n",
    "\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "          'Feature': best_features,\n",
    "          'Importance': best_pipeline['model'].feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "\n",
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()\n",
    "\n",
    "Finally, we evaluate the pipeline as usual with our custom function for classification tasks.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "We parse the arguments we are familiar with\n",
    "* Note the performance on the test set is the same as in the train set\n",
    "* for label_map, we get the classes name with .unique()\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=best_pipeline,\n",
    "                label_map= df_clf['species'].unique() \n",
    "                # in this case the target variable is encoded as categories and we\n",
    "                # get the values with .unique() \n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Refit only with most important features\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Now you know which algorithm and which hyperparameters best fit your data. \n",
    "* That is awesome! Look at your improvement and what you achieved so far.\n",
    "* However, your pipeline needs 6 columns and your model needs only 3 to predict. That means if you deploy this pipeline, your system should manage 6 inputs, when in fact you only need 3.\n",
    "* That happens since we consider a feature selection step, which is useful to determine the most appropriate features to the algorithm.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    "  In practical terms, you don't need the features that got dropped by the feature selection step. Once you know which features you can disconsider, **you can fit a new pipeline with only the most important features**.\n",
    "* This new pipeline will be deployed and contains an algorithm and hyperparameters that best suit your data and has the correct number of features \n",
    "\n",
    "These are the most important features according to the previous analysis\n",
    "\n",
    "best_features\n",
    "\n",
    "We will use the same workflow, but now using the `best_features` only,  for the train and test sets\n",
    "\n",
    "We split the data into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> And subset the `best_features` !!!\n",
    "\n",
    "X_train = X_train.filter(best_features)\n",
    "X_test = X_test.filter(best_features)\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "X_train.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You will need to update your pipeline, since you have less variables to consider and you don't need feature selection\n",
    "* Before you had 3 steps for data cleaning and feature engineering\n",
    "* Now, you have 2 steps: one for median imputation and another for categorical encoding\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Data Cleaning and Feature Engineering\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "  pipeline_base = Pipeline([\n",
    "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
    "                                     variables=['bill_length_mm' , 'flipper_length_mm']) ),\n",
    "\n",
    "      ( \"ordinal\",OrdinalEncoder(encoding_method='arbitrary', variables = ['island']) ), \n",
    "\n",
    "      (\"feat_scaling\", StandardScaler() ),\n",
    "\n",
    "      # no feature selection!!!\n",
    "\n",
    "      (\"model\", model ),\n",
    "\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "\n",
    "\n",
    "We now list the model that performed best, in this case ExtraTreesClassifier\n",
    "\n",
    "models_search = {\n",
    "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=0),\n",
    "}\n",
    "models_search\n",
    "\n",
    "We will need to hardcode the best parameters, so let's remind ourselves of the best params.\n",
    "\n",
    "grid_search_pipelines[best_model].best_params_\n",
    "\n",
    "We need to parse the value between brackets `[ ]`\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesClassifier\":{'model__n_estimators': [20]\n",
    "                            },\n",
    "\n",
    "}\n",
    "params_search\n",
    "\n",
    "We fit the model using `HyperparameterOptimizationSearch` considering the model \"ExtraTreeClassifier\" and the parameters we set previously.\n",
    "* The goal here is not to do a hyperparameter optimization search, but instead to fit a pipeline using the algorithm and best hyperparameter configuration we discovered\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-1,\n",
    "           cv=2)\n",
    "\n",
    "As usual, we check the search summary with the method .score_summary()\n",
    "* Note the performance is the same as the previous pipeline\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "We get the best model programatically\n",
    "\n",
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model\n",
    "\n",
    "So we can grab the pipeline\n",
    "\n",
    "best_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The best pipeline is a tree based algorithm, so we can check the most important features with `.feature_importances_`\n",
    "* The code is similar to the previous section, the difference is that now we don't have 3 steps in the pipeline related to data cleaning and feature engineering. Instead, we have 2 steps now\n",
    "\n",
    "data_cleaning_feat_eng_steps = 2\n",
    "\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "best_features = columns_after_data_cleaning_feat_eng\n",
    "\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "          'Feature': best_features,\n",
    "          'Importance': best_pipeline['model'].feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "\n",
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()\n",
    "\n",
    "We parse the arguments we are familiar with to evaluate the classifier performance\n",
    "* Note the performance from this pipeline is the same as from the previous pipeline - as we should expect!\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=best_pipeline,\n",
    "                label_map= df_clf['species'].unique() \n",
    "                # in this case the target variable is encoded as categories and we\n",
    "                # get the values with .unique() \n",
    "                )\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%209-%20Well%20done.png\"> Well done!\n",
    "* In this notebook you learned how to conduct a hyperparameter optimization search fitting multiple algorithms with the best features that predict penguins species\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Bonus: Most common Hyperparameters for the algorithms we cover in the course\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " It will talke **time and experience** to learn which hyperparameters to consider when optimizing your pipeline and which values would make sense to tune\n",
    "* The key is to understand how the algorithm works, and that will take time and experience. We offer here the most common hyperparameters for the algorithms we cover in the course. So you can use them as a starting point and as a reference if you require them for the Milestone Project or in the workplace.\n",
    "* Once again: the **library documenation** is your best friend to instruct you on the available hyperparmeters the library offers for that given algorithm.\n",
    "\n",
    "\n",
    "* The hyperparameters we list here are a suggestion so that you can use them as a reference when you start fine-tuning your ML pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> We will write the hyperparameters for all algorithms using the same dictionary structure we saw over the notebook, assuming you are arranging everything into a pipeline and the last step is called `'model'`\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Linear Regression\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Linear Regression doesn't have hyperparameters. You should parse an empty dictionary\n",
    "params_search = {\n",
    "    \"LinearRegression\":{},\n",
    "}\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Logistic Regression\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "params_search = {\n",
    "    \"LogisticRegression\":{'model__penalty': [\"l2\",\"l1\", \"elasticnet\"],\n",
    "                          'model__C': [1, 0.5, 2],\n",
    "                          'model__tol': [1e-4,1e-3,1e-5],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Decision Tree\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeClassifier\":{'model__max_depth': [None,4, 15],\n",
    "                              'model__min_samples_split': [2,50],\n",
    "                              'model__min_samples_leaf': [1,50],\n",
    "                              'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"DecisionTreeRegressor\":{'model__max_depth': [None,4, 15],\n",
    "                             'model__min_samples_split': [2,50],\n",
    "                             'model__min_samples_leaf': [1,50],\n",
    "                             'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Random Forest\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"RandomForestRegressor\":{'model__n_estimators': [100,50, 140],\n",
    "                             'model__max_depth': [None,4, 15],\n",
    "                             'model__min_samples_split': [2,50],\n",
    "                             'model__min_samples_leaf': [1,50],\n",
    "                             'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"RandomForestClassifier\":{'model__n_estimators': [100,50,140],\n",
    "                             'model__max_depth': [None,4, 15],\n",
    "                             'model__min_samples_split': [2,50],\n",
    "                             'model__min_samples_leaf': [1,50],\n",
    "                             'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Gradient Boosting\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "params_search = {\n",
    "    \"GradientBoostingClassifier\":{'model__n_estimators': [100,50,140],\n",
    "                                  'model__learning_rate':[0.1, 0.01, 0.001],\n",
    "                                  'model__max_depth': [3,15, None],\n",
    "                                  'model__min_samples_split': [2,50],\n",
    "                                  'model__min_samples_leaf': [1,50],\n",
    "                                  'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"GradientBoostingRegressor\":{'model__n_estimators': [100,50,140],\n",
    "                                  'model__learning_rate':[0.1, 0.01, 0.001],\n",
    "                                  'model__max_depth': [3,15, None],\n",
    "                                  'model__min_samples_split': [2,50],\n",
    "                                  'model__min_samples_leaf': [1,50],\n",
    "                                  'model__max_leaf_nodes': [None,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "####  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Ada Boost\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"AdaBoostClassifier\":{'model__n_estimators': [50,25,80,150],\n",
    "                          'model__learning_rate':[1,0.1, 2],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"AdaBoostRegressor\":{'model__n_estimators': [50,25,80,150],\n",
    "                          'model__learning_rate':[1,0.1, 2],\n",
    "                          'model__loss':['linear', 'square', 'exponential'],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "####  <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> XG Boost\n",
    "\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "params_search = {\n",
    "    \"XGBRegressor\":{'model__n_estimators': [30,80,200],\n",
    "                    'model__max_depth': [None, 3, 15],\n",
    "                    'model__learning_rate': [0.01,0.1,0.001],\n",
    "                    'model__gamma': [0, 0.1],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"XGBClassifier\":{'model__n_estimators': [30,80,200],\n",
    "                      'model__max_depth': [None, 3, 15],\n",
    "                      'model__learning_rate': [0.01,0.1,0.001],\n",
    "                      'model__gamma': [0, 0.1],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> ExtraTree\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesClassifier\":{'model__n_estimators': [100,50,150],\n",
    "                          'model__max_depth': [None, 3, 15],\n",
    "                          'model__min_samples_split': [2, 50],\n",
    "                          'model__min_samples_leaf': [1,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesRegressor\":{'model__n_estimators': [100,50,150],\n",
    "                          'model__max_depth': [None, 3, 15],\n",
    "                          'model__min_samples_split': [2, 50],\n",
    "                          'model__min_samples_leaf': [1,50],\n",
    "                            }\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
