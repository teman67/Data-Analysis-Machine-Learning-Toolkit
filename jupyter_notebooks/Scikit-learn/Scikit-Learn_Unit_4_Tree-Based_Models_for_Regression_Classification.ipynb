{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 04 - Tree-based models for Regression and Classification\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Implement and Evaluate Tree-Based Models for Regression and Classification\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 04 - Tree-based models for Regression and Classification\n",
    "\n",
    "In this unit, we will cover the practical steps and code to fit a pipeline considering Tree-based models, like Decision Trees, Random Forest.\n",
    "* In case you want to refresh the content, revert to Module 2.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> A typical workflow used for supervised learning is\n",
    "* Split the dataset into train and test set\n",
    "* Fit the pipeline\n",
    "* Evaluate your model. If the performance is not good, revisit the process, starting from defining the business case, collecting the data, conducting EDA (Exploratory Data Analysis) etc\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> For teaching purposes, **we will use a fixed dataset for Regression and a fixed dataset for Classification across the different algorithms used in this notebook.**\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will use the Boston dataset from sklearn for the **Regression task**. \n",
    "* It has house price records and house characteristics, like the average number of rooms per dwelling and the per capita crime rate in Boston.\n",
    "* We'll use the same code from the previous unit\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df_reg = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_reg['price'] = pd.Series(data.target)\n",
    "\n",
    "df_reg = df_reg.sample(frac=0.5, random_state=101)\n",
    "\n",
    "print(df_reg.shape)\n",
    "df_reg.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will use the Iris dataset from seaborn for the **Classification task**. \n",
    "* It contains records of 3 classes of iris plants, with their petal and sepal measurements\n",
    "\n",
    "df_clf = sns.load_dataset('iris').sample(frac=0.7, random_state=101)\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " We will cover the following tree algorithms, which includes ensemble tree algorithms\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Gradient Boosting\n",
    "* Ada Boost\n",
    "* XG Boost (eXtreme Gradient Boost)\n",
    "* Extra Tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For teaching purposes, we will use:\n",
    "* **Classification** task for: Decision Tree, Gradient Boosting, XG Boost\n",
    "* **Regression** task for: Random Forest, Ada Boost, Extra Tree\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> That speeds up our learning process. And, if you do Regressor using a Decision Tree, the code and workflow are the same as you would do for Classification using a Decision Tree.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Decision Tree\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may revert to module 2 - ML Essentials - on Algorithms lesson to refresh the algorithms we cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "* In a nutshell, a decision tree is like a flow chart where each question has a yes/no answer. This brings you from a general question to a very specific question as you get deeper. The questions asked must be ones where the yes or no answer gives useful insights into the data\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Depending on your task (Regression or Classification) for using Decision Tree algorithm in Sckit learn, you will import a different estimator.\n",
    "* There is the suffix \"`Regressor`\" in the estimator when the algorithm will be used for a regression task, and, as you may expect, there is the suffix \"`Classifier`\" in the estimator when the algorithm is used for the classification task.\n",
    "* That pattern repeats for the other tree-based algorithm.\n",
    "* The difference is subtle, however, it is worth pointing out.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Find here the documentation for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "* We will import both but will use the `DecisionTreeClassifier` for the exercise.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "Let's reinspect our data again.\n",
    "* The target variable is 'species' and we don't have missing data.\n",
    "\n",
    "df_clf.head()\n",
    "\n",
    "We are getting more comfortable with ML, but it is worth remembering this exercise is an example of supervised learning, where the ML task is classification. The same principle applies when the ML task is Regression.\n",
    "* For that workflow, it is wise to split the data into train and test set\n",
    "* In the previous units, we explained the `train_test_split() `function. From now on, we will just state \"We split the data into train and test sets\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "Considering the existing data, we will not need the data cleaning or the feature engineering steps.\n",
    "* We then set feature scaling, feature selection and modelling using the DecisionTreeClassifier. We set random_state, so the results will be reproducible anywhere. We chain these steps in a sklearn Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def pipeline_decision_tree_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "\n",
    "      ( \"feat_selection\",SelectFromModel(DecisionTreeClassifier(random_state=101)) ),\n",
    "      \n",
    "      ( \"model\", DecisionTreeClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "pipeline_decision_tree_clf()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> It is time to fit the pipeline, so the model can learn the relationships between the features and the target. We create a variable pipeline (it could be any name) and call the function where we set our pipeline\n",
    "\n",
    "pipeline = pipeline_decision_tree_clf()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "Like in the previous notebook, we are now interested in starting to evaluate the pipeline. Since it is a tree-based model, we can assess the importance of the features in the model using `.features_importance_`\n",
    "* We created a custom function to assess feature importance on tree-based models, it takes the model and the variables that \"hit\" the model, check the pseudo-code to understand the logic.\n",
    "* Don't worry if, at first, you don't understand.  Expect it to take some time to absorb.\n",
    "\n",
    "def feature_importance_tree_based_models(model, columns):\n",
    "  \"\"\"\n",
    "  # Gets the model, and the columns used to train the model\n",
    "  - we use the model.feature_importances_ and columns to make a\n",
    "  DataFrame that shows the importance of each feature\n",
    "  - next, we print the features name and its relative importance order,\n",
    "  followed by a barplot indicating the importance\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # create DataFrame to display feature importance\n",
    "  df_feature_importance = (pd.DataFrame(data={\n",
    "      'Features': columns,\n",
    "      'Importance': model.feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "  best_features = df_feature_importance['Features'].to_list()\n",
    "\n",
    "  # Most important features statement and plot\n",
    "  print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "        f\"The model was trained on them: \\n{df_feature_importance['Features'].to_list()}\")\n",
    "\n",
    "  df_feature_importance.plot(kind='bar',x='Features',y='Importance')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "Let's check that.\n",
    "* The `model` argument is the 'model' step from the pipeline (we don't parse the pipeline, since we need only the model step)\n",
    "* In the `columns` argument, we subset the feature selection step where we grab a boolean array informing which features hit the model - pipeline['feat_selection'].get_support(). This array is used to subset the features from train set columns.\n",
    "* Note that only 2 features - `['petal_width', 'petal_length']` - out of 4, were used to train the model and they have roughly similar relevance\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()]\n",
    "                                     )\n",
    "\n",
    "It is time to evaluate the classifier. We are using the same custom function for evaluating the classifier considered in the last notebook. \n",
    "\n",
    "# loads confusion_matrix and classification_report from sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "  \"\"\"\n",
    "  # Gets features, target, pipeline and how labelled (named) the levels from your target\n",
    "\n",
    "  - it predicts based on features\n",
    "  - compare predictions and actual in a confusion matrix\n",
    "    - the first argument stays as rows and the second stays as columns in the matrix\n",
    "    - we will use the pattern where the predictions are in the row and actual values are in the columns\n",
    "    - to refresh that, revert to the Performance Metric video in Module 2\n",
    "  - show classification report\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  \"\"\"\n",
    "  # gets the features and target from train and test set, pipeline how\n",
    "  you labelled (named) the levels from your target\n",
    "  - for each set (train and test), it calls the function above to show the confusion matrix\n",
    "  and classification report for both train and test set\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "You will notice that in this dataset, the target variable wasn't a set of numbers that refer to classes, but instead, are strings\n",
    "* We are parsing, from df_clf, the unique values from the target as the `label_map` parameter\n",
    "\n",
    "df_clf['species'].unique()\n",
    "\n",
    "Let's evaluate the classifier then\n",
    "* Note the model aced all predictions in the train set, which is an indication that it learned all the relationships from the training data. That is good, but let's check on the test set\n",
    "* As we may expect, on the test set the performance was a bit lower (we noticed that in the confusion matrix, where  Virginica and Versicolor have the wrong predictions). At the same time, it is still very good, and it is not much of a difference from the train set. It is a good indication that the model didn't overfit \n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique()\n",
    "                )\n",
    "\n",
    "One additional aspect when using DecisionTree, is to visualize the created tree.\n",
    "* Sckit learn has `plot_tree()` function that is okay and can help us, the documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html). We parse:\n",
    "* decision_tree as the model step in our pipeline\n",
    "* feature_names as the variable used to train the model. That is done by extracting the information from the feature selection step\n",
    "* class_names are taken from unique values from species\n",
    "* The remaining arguments help us to get a cleaner visualization\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> Just a side note, this decision tree is simple, however, when it comes to big trees, the visualization might become too big or more difficult to interpret.\n",
    "* In this example the decision is made first on petal_width, if it is smaller than -0.47, it is Setosa, if not it goes to another decision-making point. The other decision is for petal_lenght, if it is smaller than -0.57, it is Virginica, otherwise is Versicolor.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **Note the beauty: the algorithm computed by itself the pattern and now can predict. That is the major difference between ML and traditional programming. Here, we have data and an objective (predict species), then the computer finds the best rule for that. In traditional programming, the developer has to set the rules**\n",
    "\n",
    "* However the decision points are still weird, what does a 0.47 mean for petal_width? Negative value. Let's explore the next cell\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "tree.plot_tree(decision_tree = pipeline['model'], \n",
    "               feature_names = X_train.columns[pipeline['feat_selection'].get_support()],\n",
    "               class_names = df_clf['species'].unique(),\n",
    "               filled=True,\n",
    "               rounded=True,\n",
    "               fontsize=9,\n",
    "               impurity=False)\n",
    "plt.show()\n",
    "\n",
    "The negative values from the previous case happen due to the feature scaling step, where it scaled the data using a standard scaler. We can grab this pipeline step and use .inverse_transformation() to convert the scaled value to the original.\n",
    "* We create a dataframe that relates to the orginal data. For petal_width and petal_length we set the decision points from the previous map. We parse the dataframe to .inverse_transform\n",
    "* The decision points are actually 5.4 for petal_width, 3.3 for petal_length\n",
    "\n",
    "scaled_data = pd.DataFrame(data={'petal_width':-0.472,\n",
    "                                 'petal_length':0.578,\n",
    "                                 'sepal_length':1.0, # this value doesn't matter, but needs to be here\n",
    "                                 'sepal_width':1.0}, # this value doesn't matter, but needs to be here\n",
    "                           index=[0])\n",
    "\n",
    "\n",
    "pipeline['feat_scaling'].inverse_transform(scaled_data)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Random Forest\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may revert to module 2 - ML Essentials - on Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* The random forest is made of many decisions trees and it is an ensemble method. It uses bagging and feature randomness when building each individual tree, aiming to create an uncorrelated collection of trees, where the prediction from the set of trees is more accurate than that of any individual tree.\n",
    "\n",
    "\n",
    "\n",
    "Once again, the same algorithm has a different estimator depending on the tasks: Regression or Classification. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "* We will import both but will use `RandomForestRegressor` for the exercise.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    " We will use the Boston dataset to fit an ML pipeline to predict the sales price using the Random Forest Algorithm\n",
    "\n",
    "df_reg.head()\n",
    "\n",
    "We split the train and  test sets. The target variable is 'price' \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using a similar structure as the previous example. There are 3 steps: scaling, feature selection and modelling. \n",
    "* We know in advance the data won't need data cleaning\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def pipeline_random_forest_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestRegressor(random_state=101)) ),\n",
    "      ( \"model\", RandomForestRegressor(random_state=101)),\n",
    "\n",
    "  ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "pipeline_random_forest_reg()\n",
    "\n",
    "We will fit the pipeline to the train set (features and target) using `.fit()`\n",
    "\n",
    "\n",
    "pipeline = pipeline_random_forest_reg()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "Since it is a tree-based model, we can assess in the model the importance of the features with .features_importance_, using the custom function from the previous section\n",
    "* Note that from 13 features, the model was trained on 2: LSTAT and RM, where LSTAT is more important to the model\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "We will evaluate the regressor pipeline using the same custom function from the last unit notebook \n",
    "\n",
    "# import regression metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "# we will use numpy to calcuate RMSE based on MSE (mean_squared_error)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets train/test sets and pipeline and evaluate the performance\n",
    "  - for each set (train and test) call regression_evaluation()\n",
    "  which will evaluate the pipeline performance\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Model Evaluation \\n\")\n",
    "  print(\"* Train Set\")\n",
    "  regression_evaluation(X_train,y_train,pipeline)\n",
    "  print(\"* Test Set\")\n",
    "  regression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X,y,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets features and target (either from train or test set) and pipeline\n",
    "  - it predicts using the pipeline and the features\n",
    "  - calculates performance metrics comparing the prediction to the target\n",
    "  \"\"\"\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test,pipeline, alpha_scatter=0.5):\n",
    "  \"\"\"\n",
    "  # Gets Train and Test set (features and target), pipeline, and adjust dots transparency \n",
    "  at scatter plot\n",
    "  - It predicts on train and test set\n",
    "  - It creates an Actual vs Prediction scatterplots, for train and test set\n",
    "  - It draws a red diagonal line. In theory, a good regressor should predict\n",
    "  close to the actual, meaning the dot should be close to the diagonal red line\n",
    "  The closer the dots are to the line, the better\n",
    "\n",
    "  \"\"\"\n",
    "  pred_train = pipeline.predict(X_train)\n",
    "  pred_test = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test , y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "* We notice that the performance on the train set is pretty good (0.95 of R2, MAE of 1.4, the actual vs prediction plot is dense around the diagonal red line), however, R2 on the test set is still ok (0.72) but much lower than on the train set, there is a notable difference. That may be a sign of overfitting.\n",
    "* We note for the actual vs predictions plots that in the train set, the dots are closer around the diagonal line than they were in the test set. That reinforces the previous point.\n",
    "* Following the diagonal line means the predictions tend to follow the actual value.\n",
    "* This pipeline was trained on the default algorithm hyperparameters (like the number of trees, max depth etc). It is a matter of making sense of the hyperparameter and its common impact on algorithm performance. We will cover how to train with multiple hyperparameters in an upcoming lesson \n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Gradient Boosting\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may revert to module 2 - ML Essentials - on Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "* Gradient boosting is a type of machine learning boosting. The idea of a boosting technique is based on building a sequence of initially weak models into increasingly more powerful models. You add the Models sequentially until no further improvements can be made. Gradient boosting aims to minimize the loss function by adding weak learners using a gradient of a loss function that captures the performance of a model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We import the algorithms. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html).\n",
    "* We will import both but will use  `GradientBoostingClassifier`for the exercise.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "Let's consider the iris dataset again for the classification task\n",
    "\n",
    "df_clf.head()\n",
    "\n",
    "As usual, we split the data into train and test sets, considering 'species' as the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "The pipeline is similar to that used in the  previous section where we considered the iris dataset.\n",
    "* There are 3 steps: feature scaling, feature selection and modelling, and here we consider the Gradient Boosting Classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "\n",
    "def pipeline_gradient_boost_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=101)) ),\n",
    "      ( \"model\", GradientBoostingClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the pipeline to the train set\n",
    "\n",
    "pipeline = pipeline_gradient_boost_clf()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And check feature importance using the same function we used previously since, for this algorithm, feature importance is assessed using the same attribute\n",
    "* Note it considers only petal_length. Note also the difference; the same data in the decision tree had 2 features as the most important features. That naturally happens since different algorithms have different mechanisms\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()]\n",
    "                                     )\n",
    "\n",
    "Let's evaluate the data using the same custom function that shows the confusion matrix and classification report for the train and test sets\n",
    "* The results are the same compared to a Decision Tree (considering, the same dataset).\n",
    "* The only difference is that we needed only 1 feature to reach that result for the Gradient Boost; for the decision tree, we needed 2. So the Gradient Boost is better for this data since it is simpler and easier to have a system with fewer features.\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique()\n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Ada Boost\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may revert to module 2 - ML Essentials - on Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* AdaBoost (or Adaptive Boosting) is an ensemble learning used to build a strong model from several weak models. It uses multiple iterations to generate a single strong learner by iteratively adding weak learners. The result is a model that has higher accuracy than the weak learner itself.\n",
    "\n",
    "\n",
    "We import the algorithms. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).\n",
    "\n",
    "\n",
    "* We will import both but will use `AdaBoostRegressor` for the exercise.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    " We will use the Boston dataset to fit an ML pipeline to predict the sales price using the Ada Boost Algorithm\n",
    "\n",
    "df_reg.head()\n",
    "\n",
    "We split the train and test sets. The target variable is 'price'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using the same steps as previously but now considering the Ada Boost Regressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "def pipeline_adaboost_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(AdaBoostRegressor(random_state=101)) ),\n",
    "      ( \"model\", AdaBoostRegressor(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the data to the train set (in the same manner we did previously)\n",
    "\n",
    "pipeline = pipeline_adaboost_reg()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And assess feature importance using our custom function\n",
    "* Note this pipeline selects 3 variables to train the model: `['LSTAT', 'RM', 'DIS']`\n",
    "\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "We now evaluate the data using the custom function. \n",
    "* The R2 score on the train set is 0.9 and on the test set is 0.78. Ideally, it could be less, but this difference is lower than the difference we see for Random Forest\n",
    "* We note for the actual vs predictions plots, that in the train set, the dots are around the diagonal line (not so close as in the Random Forest). \n",
    "* Following the diagonal line means the predictions tend to follow the actual value.\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  XG Boost\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may revert to module 2 - ML Essentials - on Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* XGBoost stands for eXtreme Gradient Boosting and is an extension to gradient boosted decision trees, specially designed to improve speed and performance. It has regularization features that help to avoid over-fitting. It is a dedicated software library that you should install, it doesn't belong to the Sckit-learn library.\n",
    "\n",
    "\n",
    "We import the algorithms. Find [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) the documentation for both\n",
    "* We will import both but will use `XGBClassifier` for the exercise.\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "Let's consider the iris dataset again for the classification task\n",
    "\n",
    "df_clf.head()\n",
    "\n",
    "Let's split the data into train and test sets, where the target variable is 'species' \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['species'],axis=1),\n",
    "                                    df_clf['species'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using the same steps as previously but now considering XGBoost\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def pipeline_xgboost_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(XGBClassifier(random_state=101)) ),\n",
    "      ( \"model\", XGBClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the pipeline to the train data\n",
    "\n",
    "pipeline = pipeline_xgboost_clf()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And assess the feature importance\n",
    "* Note only petal_length is relevant to fit the model. \n",
    "\n",
    "\n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()]\n",
    "                                     )\n",
    "\n",
    "Let's assess the pipeline performance\n",
    "* The performance is the same as Gradient Boost on the train and test set. So for Classification, of the three algorithms we tested, decision tree, gradient boost, and XG boost - the last 2 are good candidates and best suit the data. However, we will study another method to test more algorithms simultaneously and avoid this segregated analysis we are doing now.\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= df_clf['species'].unique() \n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  ExtraTree\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You may revert to module 2 - ML Essentials - on Algorithms lesson to refresh the algorithms we will cover. We are not going deep into the mathematical functions; the idea is to present the concept and the algorithm application.\n",
    "\n",
    "\n",
    "* Extra Trees (or Extremely Randomized Trees) is an ensemble algorithm. It works by creating a large number of unpruned trees. Predictions are made by averaging the prediction of the decision trees when it is regression or using majority voting when it is classification.\n",
    "\n",
    "\n",
    "We import the algorithms. Find the documentation here for both, [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html).\n",
    "* We will import both but will use `ExtraTreesRegressor` for the exercise.\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "We will use the Boston dataset to fit an ML pipeline to predict the sales price\n",
    "\n",
    "df_reg.head()\n",
    "\n",
    "Let's split the data into train and test sets using 'price' as a target variable \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['price'],axis=1),\n",
    "                                    df_reg['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "We create the pipeline using the same steps as previously but now considering the Extra Tree Regressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def pipeline_extra_tree_reg():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(ExtraTreesRegressor(random_state=101)) ),\n",
    "      ( \"model\", ExtraTreesRegressor(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "We fit the pipeline to the train set\n",
    "\n",
    "pipeline = pipeline_extra_tree_reg()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "And evaluate feature importance using our custom function\n",
    "* It used `['LSTAT', 'RM']` and LSTAT is more important.\n",
    "* Just to reinforce, different algorithms consider different features to find patterns in the data, Random Forest selected the same features, and Ada Boost added to the selected list the variable DIS \n",
    "\n",
    "feature_importance_tree_based_models(model = pipeline['model'],\n",
    "                                     columns =  X_train.columns[pipeline['feat_selection'].get_support()])\n",
    "\n",
    "Let's now evaluate the pipeline\n",
    "* Note the pipeline was perfect on the train set (R2 score of 1), and on the test set was poor (R2 score of 0.68 is poor compared to a score of 1 in the train set)\n",
    "* This is a sign the model overfits since it performs better in the train set and doesn't generalize well to other sets, like the test set\n",
    "* After all, for this dataset and among Random Forest, Ada Boost and Extra Tree, Ada Boost performed better since it can generalize better (the difference between performance on train and test set is smaller).\n",
    "* Again, we analyse each algorithm separately for learning purposes; in the next unit, we will learn how to evaluate all the algorithms simultaneously.\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test,pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
