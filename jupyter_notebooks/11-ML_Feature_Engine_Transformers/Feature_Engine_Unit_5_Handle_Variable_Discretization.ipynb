{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 05 - Handle Variable Discretization\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Handle Variable Discretization using Equal Frequency discretizer, Equal Width discretizer or Arbitrary discretizer\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Variable Discretization\n",
    "\n",
    "This technique consists of transforming continuous numerical variables into discrete variables. The discrete variables will contain intervals related to the numerical distribution. The interval will be decided based on the frequency or width. We will study:\n",
    "* EqualFrequencyDiscretiser\n",
    "* EqualWidthDiscretiser\n",
    "* ArbitraryDiscretiser\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> When should I consider using them? We can consider these use cases:\n",
    "* Eventually, your feature has an abnormal or weird numerical distribution and by discretizing this variable, the categorical distribution is better understood by the model\n",
    "* You have a continuous target variable, and you are not successful in fitting a model to the dataset. Then, you can discretize the target variable and convert the ML task to classification since your target variable is now categorical. The expectation is that we will create more conditions to find a model that fits the data.\n",
    "\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Equal Frequency\n",
    "\n",
    "It divides continuous numerical variables into contiguous equal frequency intervals, that is, intervals that contain approximately the same proportion of observations. The function documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/discretisation/EqualFrequencyDiscretiser.html)\n",
    "* The arguments are `variables` to apply the method, if you don't parse anything, it will select all numerical variables. And `q` (for quantiles), which is the desired number of equal frequency intervals (or quantiles).\n",
    "\n",
    "\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "\n",
    "We will use the target variable from the Boston dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.target, columns=['target'])\n",
    "df.head()\n",
    "\n",
    "We assess the distribution with sns.histplot()\n",
    "\n",
    "sns.histplot(data=df, x='target', kde=True)\n",
    "plt.show()\n",
    "\n",
    "We create a pipeline with `EqualFrequencyDiscretiser()`, on the target variable and look for 5 bins. We then `.fit_transform()` the data\n",
    "* In the workplace, you will consider criteria to select a number for ``q``. Eventually, it will make sense to have 3 or 6. At the same time, you can run multiple simulations and assess the results for numerous ``q``\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('efd', EqualFrequencyDiscretiser(q=5, variables=['target'] ))\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "We assess the efd step and check what were the bins the transformer calculated with `.binner_dict_`\n",
    "\n",
    "pipeline['efd'].binner_dict_\n",
    "\n",
    "Finally, we plot the new target distribution. As we may expect, all intervals have the same frequency\n",
    "* Note in the plot, the bar where the target is zero, it correspond to the numerical interval of -inf to 15.3. You can extend this for the remaining bars\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The upside of using this technique, considering you are using the target variable, is that your target variable for the classification task will be already balanced, which means the labels have similar frequencies.\n",
    "\n",
    "sns.countplot(data=df_transformed, x='target')\n",
    "plt.show()\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Equal Width\n",
    "\n",
    "This technique divides continuous numerical variables into intervals of the same width. Note that the count of observations per interval may vary. The function documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/discretisation/EqualWidthDiscretiser.html).\n",
    "* The arguments are `variables` to apply the method to, if you don't parse anything, it will select all numerical variables. And `bins` which is the number of equal-width intervals/bins you want.\n",
    "\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser\n",
    "\n",
    "We will use the target variable from the Boston dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.target, columns=['target'])\n",
    "df.head()\n",
    "\n",
    "We assess its distribution\n",
    "\n",
    "sns.histplot(data=df, x='target', kde=True)\n",
    "plt.show()\n",
    "\n",
    "We create a pipeline with `EqualWidthDiscretiser()`, on the target variable and look for 6 bins. We then `.fit_transform()` the data\n",
    "* In the workplace, you will consider criteria to select a number of `bins`. Eventually, it will make sense to have 3, or 6. At the same time, you can run multiple simulations and assess the results for numerous `bins`\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('ewd', EqualWidthDiscretiser(bins=6, variables=['target']) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "We assess the ewd step and check what were the bins the transformer calculated with `.binner_dict_`\n",
    "\n",
    "pipeline['ewd'].binner_dict_\n",
    "\n",
    "Finally, we plot the new target distribution. As we may expect, all intervals have the same frequency\n",
    "* Note in the plot, the bar where the target is zero, it correspond to the numerical interval of -inf to 12.5.  You can extend this for the remaining bars\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The downside of using this technique, considering you are using the target variable, is that your target variable for the classification task will likely not be balanced.\n",
    "\n",
    "sns.countplot(data=df_transformed, x='target')\n",
    "plt.show()\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Arbitrary Discretiser\n",
    "\n",
    "It divides continuous range intervals which limits are determined by the user. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/discretisation/ArbitraryDiscretiser.html). The used argument is:\n",
    "* ``binning_dict`` is a dictionary that tells which variable you want to apply the method and the intervals.\n",
    "\n",
    "* You may use this technique when the company is comfortable with how to map the numerical values to ranges. For example, imagine if the variable is Revenue from a given purchase. The business is comfortable to assume that Revenue smaller than 100 is small, between 100 and 1000 is medium and greater than 1000 is big. You can also conduct separate analyses with other custom ranges to question current assumptions and/or look for other criteria to discretize the data\n",
    "\n",
    "from feature_engine.discretisation import ArbitraryDiscretiser\n",
    "\n",
    "We will use the target variable from the Boston dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "boston_data = datasets.load_boston()\n",
    "df = pd.DataFrame(boston_data.target, columns=['target'])\n",
    "df.head()\n",
    "\n",
    "We assess its distribution\n",
    "\n",
    "sns.histplot(data=df, x='target', kde=True)\n",
    "plt.show()\n",
    "\n",
    "We create a pipeline with `ArbitraryDiscretiser()`, on the target variable and look for 6 bins. We then `.fit_transform()` the data\n",
    "* In the workplace, you will consider criteria to select a number of `bins`. Eventually, it will make sense to have 3, or 6. At the same time, you can run multiple simulations and assess the results for numerous `bins`\n",
    "\n",
    "\n",
    "import numpy as np # we import NumPy to set -inf and +inf\n",
    "pipeline = Pipeline([\n",
    "      ( 'arbd', ArbitraryDiscretiser(binning_dict={'target':[-np.inf,10,20,40,np.inf]}) )\n",
    "])\n",
    "\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "\n",
    "We assess the arbd step and check the bins we created with `.binner_dict_`\n",
    "\n",
    "pipeline['arbd'].binner_dict_\n",
    "\n",
    "Finally, we plot the new target distribution. As we may expect, all intervals have the same frequency\n",
    "* Note in the plot, the bar where the target is zero, corresponds to the numerical interval of -inf to 10.  You can extend this for the remaining bars\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The upside of this technique is that we set the intervals we are comfortable with. The downside is that the categorical distribution may be imbalanced.\n",
    "\n",
    "sns.countplot(data=df_transformed, x='target')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
