{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 09 - Custom functions for Data Cleaning and Feature Engineering Workflow\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand and use custom functions for data cleaning and feature engineering workflow, using feature-engine transformers\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Custom functions for Data Cleaning and Feature Engineering Workflow\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You probably noticed the exercises from previous units took time and energy. There is no fixed recipe but instead guidelines.\n",
    "* This is the reason that data practitioners spend a lot of energy and time in data cleaning and feature engineering the variables\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> We created a custom function, made with specific feature-engine transformers, to help you be more effective in the Data Cleaning and Feature Engineering stage. We will instruct you on how we expect you to use and interpret it.\n",
    "\n",
    "* There are 2 functions we will present to you now, and we will use them in Walkthrough Project 02.\n",
    "  * `DataCleaningEffect()`\n",
    "  * `FeatureEngineeringAnalysis()`\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> These custom functions were delivered specially for this specialization. The functions logic and usability were tested and reviewed extensively, however bugs may appear.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* We will not focus on explaining the code itself but focus on the functionality and instruct how we could use it\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " `DataCleaningEffect()`\n",
    "* Function objective: assess the effect of cleaning the data, when\n",
    "  * imput mean, median or arbitrary number is a numerical variable\n",
    "  * replace with 'Missing' or most frequent a categorical variable\n",
    "* Parameters: `df_original`: data not cleaned, `df_cleaned`: cleaned data,`variables_applied_with_method`: variables where you applied a given method\n",
    "\n",
    "  * It is understandable if, at first, you don't understand all code from the function below. The point is to make sense of the pseudo-code and understand the function parameters.\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
    "\n",
    "  flag_count=1 # Indicate plot number\n",
    "  \n",
    "  # distinguish between numerical and categorical variables\n",
    "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
    "\n",
    "  # scan over variables, \n",
    "    # first on variables that you applied the method\n",
    "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
    "  for set_of_variables in [variables_applied_with_method]:\n",
    "    print(\"\\n=====================================================================================\")\n",
    "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
    "    print(f\"{set_of_variables} \\n\\n\")\n",
    "  \n",
    "\n",
    "    for var in set_of_variables:\n",
    "      if var in categorical_variables:  # it is categorical variable: barplot\n",
    "        \n",
    "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
    "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
    "        dfAux = pd.concat([df1, df2], axis=0)\n",
    "        fig , axes = plt.subplots(figsize=(15, 5))\n",
    "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"])\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend() \n",
    "\n",
    "      else: # it is numerical variable: histogram\n",
    "\n",
    "        fig , axes = plt.subplots(figsize=(10, 5))\n",
    "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\", ax=axes)\n",
    "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\", ax=axes)\n",
    "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "        plt.legend() \n",
    "\n",
    "      plt.show()\n",
    "      flag_count+= 1\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\">\n",
    " `FeatureEngineeringAnalysis()`\n",
    "* Function objective: apply a set of transformers, defined by the user, for a given set of variables\n",
    "* Parameters: `df`: data, `analysis_type`:` ['numerical', 'ordinal_encoder',  'outlier_winsorizer']`\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> You should parse the proper variable data types according to your analysis, for example, you shall parse only numerical variables when selecting 'numerical' for analysis_type\n",
    "\n",
    "  * It is understandable if, at first, you don't understand all code from the function below. The point is to make sense of the pseudo-code and understand the function parameters.\n",
    "\n",
    "from feature_engine import transformation as vt\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def FeatureEngineeringAnalysis(df,analysis_type=None):\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  - used for quick feature engineering on numerical and categorical variables\n",
    "  to decide which transformation can better transform the distribution shape \n",
    "  - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
    "\n",
    "  \"\"\"\n",
    "  check_missing_values(df)\n",
    "  allowed_types= ['numerical', 'ordinal_encoder',  'outlier_winsorizer']\n",
    "  check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
    "  list_column_transformers = define_list_column_transformers(analysis_type)\n",
    "  \n",
    "  \n",
    "  # Loop over each variable and engineer the data according to the analysis type\n",
    "  df_feat_eng = pd.DataFrame([])\n",
    "  for column in df.columns:\n",
    "    # create additional columns (column_method) to apply the methods\n",
    "    df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
    "    for method in list_column_transformers:\n",
    "      df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
    "      \n",
    "    # Apply transformers in respectives column_transformers\n",
    "    df_feat_eng, list_applied_transformers = apply_transformers(analysis_type, df_feat_eng, column)\n",
    "\n",
    "    # For each variable, assess how the transformations perform\n",
    "    transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng)\n",
    "\n",
    "  return df_feat_eng\n",
    "\n",
    "\n",
    "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
    "  ### Check analyis type\n",
    "  if analysis_type == None:\n",
    "    raise SystemExit(f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
    "  if analysis_type not in allowed_types:\n",
    "      raise SystemExit(f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
    "\n",
    "def check_missing_values(df):\n",
    "  if df.isna().sum().sum() != 0:\n",
    "    raise SystemExit(\n",
    "        f\"There is missing values in your dataset. Please handle that before getting into feature engineering.\")\n",
    "\n",
    "\n",
    "\n",
    "def define_list_column_transformers(analysis_type):\n",
    "  ### Set suffix colummns acording to analysis_type\n",
    "  if analysis_type=='numerical':\n",
    "    list_column_transformers = [\"log_e\",\"log_10\",\"reciprocal\", \"power\",\"box_cox\",\"yeo_johnson\"]\n",
    "  \n",
    "  elif analysis_type=='ordinal_encoder':\n",
    "    list_column_transformers = [\"ordinal_encoder\"]\n",
    "\n",
    "  elif analysis_type=='outlier_winsorizer':\n",
    "    list_column_transformers = ['iqr']\n",
    "\n",
    "  return list_column_transformers\n",
    "\n",
    "\n",
    "\n",
    "def apply_transformers(analysis_type, df_feat_eng, column):\n",
    "\n",
    "\n",
    "  for col in df_feat_eng.select_dtypes(include='category').columns:\n",
    "    df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
    "\n",
    "\n",
    "  if analysis_type=='numerical':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,column)\n",
    "  \n",
    "  elif analysis_type=='outlier_winsorizer':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_OutlierWinsorizer(df_feat_eng,column)\n",
    "\n",
    "  elif analysis_type=='ordinal_encoder':\n",
    "    df_feat_eng,list_applied_transformers = FeatEngineering_CategoricalEncoder(df_feat_eng,column)\n",
    "\n",
    "  return df_feat_eng,list_applied_transformers\n",
    "\n",
    "\n",
    "\n",
    "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
    "  # For each variable, assess how the transformations perform\n",
    "  print(f\"* Variable Analyzed: {column}\")\n",
    "  print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
    "  for col in [column] + list_applied_transformers:\n",
    "    \n",
    "    if analysis_type!='ordinal_encoder':\n",
    "      DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "    \n",
    "    else:\n",
    "      if col == column: \n",
    "        DiagnosticPlots_Categories(df_feat_eng, col)\n",
    "      else:\n",
    "        DiagnosticPlots_Numerical(df_feat_eng, col)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
    "  plt.figure(figsize=(20, 5))\n",
    "  sns.countplot(data=df_feat_eng, x=col,palette=['#432371'],order = df_feat_eng[col].value_counts().index)\n",
    "  plt.xticks(rotation=90) \n",
    "  plt.suptitle(f\"{col}\", fontsize=30,y=1.05)        \n",
    "  plt.show();\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def DiagnosticPlots_Numerical(df, variable):\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "  sns.histplot(data=df, x=variable, kde=True,element=\"step\",ax=axes[0]) \n",
    "  stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
    "  sns.boxplot(x=df[variable],ax=axes[2])\n",
    "  \n",
    "  axes[0].set_title('Histogram')\n",
    "  axes[1].set_title('QQ Plot')\n",
    "  axes[2].set_title('Boxplot')\n",
    "  fig.suptitle(f\"{variable}\", fontsize=30,y=1.05)\n",
    "  plt.show();\n",
    "\n",
    "\n",
    "def FeatEngineering_CategoricalEncoder(df_feat_eng,column):\n",
    "  list_methods_worked = []\n",
    "  try:  \n",
    "    encoder= OrdinalEncoder(encoding_method='arbitrary', variables = [f\"{column}_ordinal_encoder\"])\n",
    "    df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
    "  \n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_ordinal_encoder\"],axis=1,inplace=True)\n",
    "    \n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "\n",
    "def FeatEngineering_OutlierWinsorizer(df_feat_eng,column):\n",
    "  list_methods_worked = []\n",
    "\n",
    "  ### Winsorizer iqr\n",
    "  try: \n",
    "    disc=Winsorizer(\n",
    "        capping_method='iqr', tail='both', fold=1.5, variables = [f\"{column}_iqr\"])\n",
    "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_iqr\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def FeatEngineering_Numerical(df_feat_eng,column):\n",
    "\n",
    "  list_methods_worked = []\n",
    "\n",
    "  ### LogTransformer base e\n",
    "  try: \n",
    "    lt = vt.LogTransformer(variables = [f\"{column}_log_e\"])\n",
    "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_log_e\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_log_e\"],axis=1,inplace=True)\n",
    "\n",
    "    ### LogTransformer base 10\n",
    "  try: \n",
    "    lt = vt.LogTransformer(variables = [f\"{column}_log_10\"],base='10')\n",
    "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_log_10\")\n",
    "  except: \n",
    "    df_feat_eng.drop([f\"{column}_log_10\"],axis=1,inplace=True)\n",
    "\n",
    "  ### ReciprocalTransformer\n",
    "  try:\n",
    "    rt = vt.ReciprocalTransformer(variables = [f\"{column}_reciprocal\"])\n",
    "    df_feat_eng =  rt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_reciprocal\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_reciprocal\"],axis=1,inplace=True)\n",
    "\n",
    "  ### PowerTransformer\n",
    "  try:\n",
    "    pt = vt.PowerTransformer(variables = [f\"{column}_power\"])\n",
    "    df_feat_eng = pt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_power\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_power\"],axis=1,inplace=True)\n",
    "\n",
    "  ### BoxCoxTransformer\n",
    "  try:\n",
    "    bct = vt.BoxCoxTransformer(variables = [f\"{column}_box_cox\"])\n",
    "    df_feat_eng = bct.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_box_cox\")\n",
    "  except:\n",
    "    df_feat_eng.drop([f\"{column}_box_cox\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  ### YeoJohnsonTransformer\n",
    "  try:\n",
    "    yjt = vt.YeoJohnsonTransformer(variables = [f\"{column}_yeo_johnson\"])\n",
    "    df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
    "    list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
    "  except:\n",
    "        df_feat_eng.drop([f\"{column}_yeo_johnson\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "  return df_feat_eng,list_methods_worked\n",
    "\n",
    "We will present the use cases and interpretations, so you can conduct your data cleaning and feature engineering steps more effectively\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle missing data\n",
    "\n",
    "We are assuming that at this moment of your project in the workplace, you have already conducted an initial EDA of your data, and you know which variables require you to handle missing data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> In this exercise for Data Cleaning, we  will follow these steps:\n",
    "\n",
    "* 1 - Select an imputation method\n",
    "* 2 - Select variables to apply the method to\n",
    "* 3 - Create a separate DataFrame to apply the method\n",
    "* 4 - Assess the effect on the variable distribution\n",
    "\n",
    "Let's consider the titanic dataset. It holds passengers records from its unique ride. \n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url).drop(['alive'],axis=1)\n",
    "# df = sns.load_dataset('titanic').drop(['alive'],axis=1)\n",
    "df.head()\n",
    "\n",
    "We inspect the dataset and notice there are variable data types which are `'category'`. \n",
    "* Typically, categorical variables are handled as `'object'`, but sometimes, for some reason, the data was stored as `'category'` instead. \n",
    "* Feature engine library handles the data properly when a categorical variable is an `'object'` data type. \n",
    "\n",
    "df.info()\n",
    "\n",
    "We will convert them to `'object'` data type by looping over the variables with data type as `'category'` and converting to `'object'`\n",
    "\n",
    "for col in df.select_dtypes(include='category'):\n",
    "  df[col] = df[col].astype('object')\n",
    "\n",
    "We check for missing data. \n",
    "* There are numerical and categorical data with missing data\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Numerical\n",
    "\n",
    "In the methods we covered, you may impute with mean, median or arbitrary.\n",
    "* For our exercise, we will assume we made an EDA and selected median\n",
    "\n",
    "1 - Select an imputation method\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "2 - Select the variables to apply the method to\n",
    "* you have to make sure you are using numerical variables\n",
    "\n",
    "variables_method = ['age']\n",
    "variables_method\n",
    "\n",
    "3 - Create a separate DataFrame to apply the method\n",
    "\n",
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
    "df_method = imputer.fit_transform(df)\n",
    "\n",
    "\n",
    "\n",
    "4 - Assess the effect on the variable distribution\n",
    "* The function plots in the same Axes the distribution before and after applying the method. This helps to give you insights into how different your variable would look after cleaning.\n",
    "* We notice the \"peak\" in the variable distribution after median imputation.\n",
    "\n",
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Categorical\n",
    "\n",
    "In this exercise, we will impute 'Missing' on categorical variables \n",
    "\n",
    "1 - Select an imputation method\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "2 - Select variables to apply the method to\n",
    "* you have to make sure you are using categorical variables\n",
    "\n",
    "variables_method = ['embarked', 'deck', 'embark_town']\n",
    "variables_method\n",
    "\n",
    "3 - Create a separate DataFrame to apply the method\n",
    "\n",
    "imputer = CategoricalImputer(imputation_method='missing',fill_value='Missing',\n",
    "                             variables=variables_method)\n",
    "\n",
    "df_method = imputer.fit_transform(df)\n",
    "\n",
    "4 - Assess the effect on the variable distribution\n",
    "* It was probably not a good idea to consider this method on these variables\n",
    "  * For the deck, we might consider dropping the variable, since its missing levels are high\n",
    "  * For embarked and embark_town, we may consider replacing with most frequent since the missing data levels are low.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> This exercise gives an idea of how this function works in practice.\n",
    "\n",
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Engineering\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\n",
    "\"> In this exercise for Feature Engineering workflow, we will follow these steps:\n",
    "\n",
    "* 1 - Select variable(s)\n",
    "* 2 - Create a separate dataframe, for that variable(s)\n",
    "* 3 - Assess engineered variables distribution \n",
    "\n",
    "\n",
    "In your career, you will develop your preferences and unique methods for dealing with data cleaning and feature engineering. As a starting point, we suggest starting the feature engineering workflow by:\n",
    "* Looking for categorical encoding\n",
    "* Looking for handling outliers\n",
    "* Looking for numerical transformation\n",
    "\n",
    "---\n",
    "\n",
    "Let's recap our dataset\n",
    "\n",
    "df.head()\n",
    "\n",
    "We can check missing data levels\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "In the last section, we didn't impute any missing data to the original DataFrame (df), we just checked how it would look like if we applied a given imputer.\n",
    "* For the next exercise, we create a quick pipeline to manage missing data, but dropping the feature with a lot of missing data, add median as imputer for age, and drop the remaining missing data\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import DropMissingData\n",
    "\n",
    "data_cleaning_pipeline = Pipeline([\n",
    "      ( 'DropFeatures', DropFeatures(features_to_drop=['deck']) ),\n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median', variables=['age']) ),\n",
    "      ( 'DropMissingData', DropMissingData()),\n",
    "])\n",
    "\n",
    "df = data_cleaning_pipeline.fit_transform(df)\n",
    "\n",
    "We check missing data levels again\n",
    "* we are good to go for feature engineering\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Ordinal Encoder\n",
    "\n",
    "Again We will assume that at this moment, you are in a project in the workplace, you will have already done an EDA on the variables, so you will know which variables to encode.\n",
    "\n",
    "1 - Select variable(s)\n",
    "\n",
    "variables_engineering= ['sex', 'embarked', 'who', 'embark_town']\n",
    "variables_engineering\n",
    "\n",
    "2 - Create a separate dataframe, for these variables\n",
    "\n",
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(3)\n",
    "\n",
    "3 - Assess engineered variables distribution \n",
    "* We notice when we encode a category to number, the distribution will not be normal distributed. The new data type is numerical discrete (not continuous), and that is fine\n",
    "\n",
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering,analysis_type='ordinal_encoder')\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Outlier \n",
    "\n",
    "Again We will assume that at this moment, you are in a project in the workplace, you will have already done an EDA on the variables, so you will know which variables to consider in this outlier analysis\n",
    "\n",
    "1 - Select variable(s)\n",
    "\n",
    "variables_engineering = ['age', 'fare']\n",
    "variables_engineering\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "2 - Create a separate dataframe, for the variable(s)\n",
    "\n",
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(5)\n",
    "\n",
    "3 - Assess engineered variables distribution \n",
    "* We note that for both variables, replacing outliers with the IQR method didn't help to become normal distributed but helped to become less abnormal, and this tends to be positive for an ML model. Therefore, you will consider this step in your pipeline when age and fare are features. \n",
    "\n",
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering.dropna(),\n",
    "                                            analysis_type='outlier_winsorizer')\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Numerical\n",
    "\n",
    "Again We will assume that at this moment, you are in a project in the workplace, you will have already done an EDA on the variables, so you will know which variables to try numerical transformation\n",
    "\n",
    "1 - Select variable(s)\n",
    "\n",
    "variables_engineering= ['fare']\n",
    "variables_engineering\n",
    "\n",
    "2 - Create a separate dataframe, for the variable(s)\n",
    "\n",
    "df_engineering = df[variables_engineering].copy()\n",
    "df_engineering.head(3)\n",
    "\n",
    "3 - Assess engineered variables distribution \n",
    "\n",
    "* The function will try to transform a variable using the following transformer: Log base e and base 10 Transformer, Power Transformer, Reciprocal Transformer, Box Cox Transformer and Yeo Johnson Transformer. In case it is not possible to compute a given transformation (ex.: log transformation doesn't work for negative values), the function will dismiss that given transformation to that given variable.\n",
    "* For fare, it was possible only to apply Power Transformer and Yeo Johnson.\n",
    "* Yeo Johnson has a distribution with fewer outliers, and even not being normal distributed, it is better than before. We shall consider this transformer for rare features.\n",
    "\n",
    "df_engineering = FeatureEngineeringAnalysis(df=df_engineering,analysis_type='numerical')\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
