{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests - Unit 03: Nonparametric Statistical Tests\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "  * Conduct and interpret statistical tests using Mann Whitney, Wilcoxon and Kruskal Wallis test\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import pingouin as pg\n",
    "import scipy\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Statistical Tests - Unit 03: Nonparametric Statistical Tests\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> There are parametric and nonparamteric statistical tests.\n",
    "* Depending on the normality of your data (meaning if it is normal distribution or not), you may use a parametric test or a non parametric test\n",
    "\n",
    "  * If it is normally distributed, we use parametric test.\n",
    "  * If it is not, we use nonparametric test.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> **In this unit we will cover nonparametric tests**\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Mann-Whitney U Test\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  A Mann-Whitney U Test is a nonparametric test, used to determine if there are differences between two groups, where at least one group is not normally distributed\n",
    "* The samples should be independent (or unpaired).\n",
    "* A nonparametric version to the independent t-test we saw in the previous unit.\n",
    "\n",
    "Let's consider a DataFrame that has Col1 and Col2 columns made with NumPy function to create not normal distributed data\n",
    "\n",
    "np.random.seed(1)\n",
    "df = pd.DataFrame(data={'Col1':np.random.uniform(low=0, high=1, size=500),\n",
    "                        \"Col2\":np.random.uniform(low=0.1, high=1, size=500)})\n",
    "df.head()\n",
    "\n",
    "\n",
    "We check for normality. Both are not normally distributed\n",
    "\n",
    "pg.normality(data=df, alpha=0.05)\n",
    "\n",
    "We plot both columns in a histogram and boxplot to better understand the levels\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1 ,ncols=2 ,figsize=(12,5))\n",
    "sns.histplot(data=df, kde=True, ax=axes[0])\n",
    "sns.boxplot(data=df, ax=axes[1])\n",
    "plt.show()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "We use `pg.mwu()` to conduct a  Mann-Whitney U Test. The documentation link is [here](https://pingouin-stats.org/generated/pingouin.mwu.html). The arguments we use are x and y, where we parse the numerical distribution\n",
    "\n",
    "pg.mwu(x=df['Col1'], y=df['Col2'])\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are interested in p-val, which is 0.0784\n",
    "* We consider our significant level alpha = 0.05. \n",
    "* Since p-value (0.0784) is higher than alpha, we accept the null hypothesis.\n",
    "\n",
    "* Therefore there is not enough statistical difference to conclude the levels are different.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Wilcoxon Test\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  A Wilcoxon Test is a non-parametric test used when you'd like to use the paired t–test. At least one of the samples should be **not normally distributed.**\n",
    "* The samples should be **dependent (or paired)**\n",
    "  * It should be a sample of matched pairs. \n",
    "  * For example, **imagine the same group is tested twice**. Say you want to examine the difference between people's scores on a test before and after a training intervention \n",
    "\n",
    "\n",
    "\n",
    "Let's consider a DataFrame that has Col3 and Col4 columns made with a python list\n",
    "\n",
    "df = pd.DataFrame(data={'Col3':[18.3, 13.3, 16.5, 12.6, 9.5, 13.6, 8.1, 8.9, 10, 8.3, 7.9, 8.1, 13.4],\n",
    "                        \"Col4\":[12.7, 11.1, 15.3, 12.7, 10.5, 15.6, 11.2, 14.2, 16.3, 15.5, 19.9, 20.4, 36.8]\n",
    "                        })\n",
    "df.head()\n",
    "\n",
    "Let's check for normality. One of them is not normal distributed, so we can use Wilcoxon test\n",
    "\n",
    "pg.normality(data=df, alpha=0.05)\n",
    "\n",
    "We plot both columns in a histogram and boxplot to better understand the levels\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1 ,ncols=2 ,figsize=(12,5))\n",
    "sns.histplot(data=df, kde=True, ax=axes[0])\n",
    "sns.boxplot(data=df, ax=axes[1])\n",
    "plt.show()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "We use `pg.wilcoxon()` to conduct a Wilcoxon Test. The documentation is [here](https://pingouin-stats.org/generated/pingouin.wilcoxon.html). The arguments we use are x and y, as the numerical data we want to compare.\n",
    "\n",
    "pg.wilcoxon(x=df['Col3'], y=df['Col4'])\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are interested in the p-val, which is 0.0397\n",
    "* We consider our significant level alpha = 0.05. \n",
    "* Since p-value (0.0397) is lower than alpha, we reject the null hypothesis.\n",
    "\n",
    "* Therefore there is enough statistical difference to conclude the levels are different.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Kruskal-Wallis\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> A Kruskal-Wallis test is a nonparametric test used to determine if there are differences between three or more groups, considered when at least one of the distributions are not normally distributed\n",
    "* A nonparametric alternative to one-way ANOVA\n",
    "\n",
    "\n",
    "\n",
    "We use a pingouin dataset. We will be interested in Metric and Performance variables to demonstrate the concept in this exercise\n",
    "\n",
    "df= pg.read_dataset(\"rm_anova2\").filter(['Metric',\t'Performance'])\n",
    "df.head()\n",
    "\n",
    "We want to know Metric distribution to know its levels, so we use `.value_counts()`. There are 3 levels (action, product and client)\n",
    "\n",
    "df['Metric'].value_counts()\n",
    "\n",
    "We check for normality. One of them is not, so we can use Kruskal Wallis\n",
    "\n",
    "pg.normality(data=df, dv='Performance', group='Metric', alpha=0.05)\n",
    "\n",
    "We combine a boxplot and swarm plot to visually check `Performance` across different `Metric`\n",
    "* **Visually**, we notice few datapoints. And it looks to have a `Performance` difference across different `Metric`. However, it is **wise** not to conclude anything before conducting a statistical test.\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n",
    "sns.boxplot(data=df, x=\"Metric\", y=\"Performance\", ax=axes[0])\n",
    "sns.swarmplot(data=df, x=\"Metric\", y=\"Performance\", dodge=True, ax=axes[1])\n",
    "plt.show()\n",
    "\n",
    "We use `pg.kruskal()` to conduct a Kruskal Wallis test. The documentation is [here](https://pingouin-stats.org/generated/pingouin.kruskal.html#pingouin.kruskal). The arguments are data, ``dv`` as the dependent variable and ``between`` as the variable we will use to analyze the levels in between.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pg.kruskal(data=df, dv='Performance', between='Metric')\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are interested in p-unc, which is 0.00012\n",
    "* We consider our significant level alpha = 0.05. \n",
    "* Since p-value (0.00012) is lower than alpha, we reject the null hypothesis.\n",
    "\n",
    "* Therefore there is enough statistical difference to conclude the levels are different.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
