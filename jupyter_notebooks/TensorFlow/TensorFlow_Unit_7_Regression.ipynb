{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow - Unit 07 - Regression\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Fit a deep learning neural network for Regression task\n",
    "* Save and load tensorflow models (.h5 extension)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Unit 08 - Regression\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Workflow\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Challenge%20test.png\">\n",
    " We will follow the typical process for supervised learning which we are familiar with, but now with a few tweaks:\n",
    "\n",
    "* Split the dataset into train, validation and test set\n",
    "* Create a pipeline to handle data cleaning, feature engineering and feature scaling (as we covered, it is highly recommended the data be scaled, so we wrap up in one pipeline)\n",
    "* Create the neural network\n",
    "* Fit the pipeline to the train set and transformations to the other sets\n",
    "* Fit the model to the train and validation set\n",
    "* Evaluate the model\n",
    "* Predict on new data\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Load and split the data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's first load the data. We are using the Boston dataset from sklearn.\n",
    "* It has house price records and house characteristics, like the average number of rooms per dwelling and the per capita crime rate in Boston. The target variable is the house price.\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df['price'] = pd.Series(data.target)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.\n",
    "amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> As part of our workflow, we split the data, but now we will split it into train, validation, test sets. \n",
    "* First, we split into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df.drop(['price'],axis=1),\n",
    "                                    df['price'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=0\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Then, from the train set, we split a validation set. We set the validation set as 20% of the train set\n",
    "* Have a look at the print statement, which shows the amount of data we have in each set (train, validation and test)\n",
    "\n",
    "X_train, X_val,y_train, y_val = train_test_split(\n",
    "                                    X_train,\n",
    "                                    y_train,\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=0\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"* Validation set:\",  X_val.shape, y_val.shape)\n",
    "print(\"* Test set:\",   X_test.shape, y_test.shape)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pipeline for data processing\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  We first create a pipeline for preprocessing the data. In theory, it could handle the processes of data cleaning, feature engineering and feature scaling\n",
    "* In this case, it's only features scaling.\n",
    "* We could have also added a step for removing correlated features, but let's keep it simple.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "### Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pipeline_pre_processing():\n",
    "  pipeline_base = Pipeline([\n",
    "      \n",
    "      ( \"feat_scaling\",StandardScaler() )\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next, we fit the pipeline to the train set and transformations to the validation and test set\n",
    "* So the pipeline can learn the transformations (in this case it is only feature scaling) from the train set, and apply the transformation to the other sets. \n",
    "\n",
    "pipeline = pipeline_pre_processing()\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_val= pipeline.transform(X_val)\n",
    "X_test = pipeline.transform(X_test)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Create Deep Learning Network\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  We will create a tensorflow model.\n",
    "* We create a function that creates a sequential model, compiles the model and returns the mode. The function requires the number of features the data has to be used as the number of neurons from the first layer\n",
    "* Let's define the network architecture (a deep learning neural network since it has 2 or more hidden layers - jargon alert! )\n",
    "  * We noted the data has 13 features. First, we will create a simple network just for a learning experience. \n",
    "  * The network is built using Dense layers - fully connected layers\n",
    "  * The input layer has the same number of neurons as the number of columns from the data. The activation function is relu. We parse the input_shape in a tuple, the first value is the number of columns from the data, and you don't need to parse the second since the data is uni-dimensional (an image wouldn't be unidimensional, for example)\n",
    "  * We are using 2 hidden layers, the first with 8 neurons and the next with 4 neurons. Both will use relu as an activation function.\n",
    "  * After each hidden layer, we have a dropout layer with a 25% rate; so we can reduce the chance of overfitting\n",
    "  * The output layer should contain only 1 neuron since the ML task is Regression. \n",
    "  * We compile the model with mse as loss/cost function and optimizer as adam\n",
    "\n",
    "import os;\n",
    "import tensorflow as tf;\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2';\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def create_tf_model(n_features):\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Dense(units=n_features, activation='relu', input_shape=(n_features,)))\n",
    "\n",
    "  model.add(Dense(units=8,activation='relu'))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Dense(units=4,activation='relu'))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Dense(units=1))\n",
    "  model.compile(loss='mse', optimizer='adam')\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's visualize the network structure\n",
    "* Note the number of parameters the network has. That looks to be a reasonable amount compared to the number of rows the train set has\n",
    "* A non-reasonable amount would be like 100 thousand parameters for a dataset with 1k. Or maybe your dataset is so tiny and complex, and you need more parameters, but the rule of thumb suggests starting easy and adding more complexity if the performance is not good.\n",
    "\n",
    "model = create_tf_model(n_features=X_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We can use `plot_model()` also from Keras.utils for a more graphical approach\n",
    "* Note the input and output shape each layer has. That is how your data is \"travelling\" from the input to the prediction.\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Fit the model\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> As we mentioned in a previous notebook, early stopping stops training when a monitored metric has stopped improving; this is useful to avoid overfitting the model to the data. The documentation function is [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)\n",
    "* We will monitor the validation loss \n",
    "  * Remember that now we parse the train and validation data. After a given epoch finish, the network calculates the error. The training process stops if the validation error doesn't improve for a given set of consecutive epochs. \n",
    "  * We set patience as 15, which is the number of epochs with no improvement; after that, training will be stopped. Although there is no fixed rule to set patience, if you feel that your model was learning still, then you stopped, you may increase the value and train again.\n",
    "  * We set the mode to min. According to TensorFlow documentation, in min mode, training will stop when the quantity monitored has stopped decreasing.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We finally will fit the model\n",
    "* We create the model object and use .fit(), as usual\n",
    "  * We parse the Train set\n",
    "  * The epoch is set to 100. In theory, you may set a high value since we will add an early stop, which stops the training process when there is no training improvement. \n",
    "  * We parse the validation data in a tuple.\n",
    "  * Verbose is set to 1 to see in which epochs we are and the training and validation loss.\n",
    "  * Finally, we parse our callback as the early_stop object we created earlier. We parse in a list since you may parse more than 1 type of callback. In this course, we will cover only early stopping\n",
    "\n",
    "* For each epoch, note the training and validation loss. Are they increasing? Decreasing? Still?\n",
    "  * Ideally, it should decrease as long as the epoch increases, showing a practical sign the network is learning\n",
    "\n",
    "\n",
    "model = create_tf_model(n_features=X_train.shape[1])\n",
    "\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=100,\n",
    "          validation_data=(X_val, y_val),\n",
    "          verbose=1,\n",
    "          callbacks=[early_stop]\n",
    "          )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Model evaluation\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  Now we will evaluate the model performance by analyzing the train and validation losses that happened during the training process. \n",
    "* In deep learning we use the model history to assess if the model learned, using the train and validation sets. We also evaluate separately how the model generalize on unseen data (on the test set)\n",
    "* The model training history information is stored in a `.history.history` attribute. Note it shows a loss (training loss) and val_loss (validation_loss)\n",
    "\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "losses\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are plotting each loss in a line plot, where the y-axis has the loss value, the x-axis is the epoch number and the lines are colored by train or validation\n",
    "* We use `.plot(style='.-')` for this task\n",
    "* Note the loss plots for training and validation data follow a similar path and are close to each other. It looks like the network learned the patterns.\n",
    "\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "losses[['loss','val_loss']].plot(style='.-')\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Next, we will evaluate the model performance on the test set, using `.evaluate()` and parsing the test set. Note the value is not much different from the losses in the train and validation set.\n",
    "* Note the model learned the relationship between the features and the target, considering all features. Conventional ML often use a feature selection step to remove features that wouldn't contribute to the model learning, thus increasing the chance of overfitting the model.\n",
    "* But in Deep Learning, the neural network handles this topic by itself. The connections that are related to features with less importance are weaker after the training process; therefore, it doesn't harm the overall performance.\n",
    "\n",
    "model.evaluate(X_test,y_test)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> When evaluating a deep learning model you typically cover the loss plot and evaluate the test set, however, **you can do if you want as an additional step** a similar evaluation we did in conventional ML.\n",
    "* In regression, you would analyze the performance metrics and actual x predictions plot, using the custom function we have seen over the course.\n",
    "* One difference is that we readapted the function also to evaluate the validation set, but that is a minor change in the code; the overall logic is the same\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "import numpy as np\n",
    "\n",
    "def regression_performance(X_train, y_train,\n",
    "                           X_val, y_val,\n",
    "                           X_test, y_test,pipeline):\n",
    "\n",
    "  print(\"Model Evaluation \\n\")\n",
    "  print(\"* Train Set\")\n",
    "  regression_evaluation(X_train,y_train,pipeline)\n",
    "  print(\"* Validation Set\")\n",
    "  regression_evaluation(X_val, y_val,pipeline)\n",
    "  print(\"* Test Set\")\n",
    "  regression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X,y,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets features and target (either from train or test set) and pipeline\n",
    "  - it predicts using the pipeline and the features\n",
    "  - calculates performance metrics comparing the prediction to the target\n",
    "  \"\"\"\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train,\n",
    "                                X_val, y_val,\n",
    "                                X_test, y_test,\n",
    "                                pipeline, alpha_scatter=0.5):\n",
    "\n",
    "  pred_train = pipeline.predict(X_train).reshape(-1) \n",
    "  # we reshape the prediction arrays to be in the format (n_rows,), so we can plot it after\n",
    "  pred_val = pipeline.predict(X_val).reshape(-1)\n",
    "  pred_test = pipeline.predict(X_test).reshape(-1)\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_val , y=pred_val, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_val , y=y_val, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Validation Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test , y=pred_test, alpha=alpha_scatter, ax=axes[2])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[2])\n",
    "  axes[2].set_xlabel(\"Actual\")\n",
    "  axes[2].set_ylabel(\"Predictions\")\n",
    "  axes[2].set_title(\"Test Set\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's parse the values as usual.\n",
    "* Note here we don't parse a pipeline, we are the TensorFlow model\n",
    "* Note the predictions tend to follow the red diagonal line. However, it seems the test set metrics are quite different from the train/validation set. You could add more complexity to the model, or increase the number of epochs etc until you reach a metric that can satisfy you. For learning purposes, we will be happy with this performance. \n",
    "* In general, we would expect the performance to be better in the train set, then validation, then test set. However, there may be cases where this doesn't happen.\n",
    "\n",
    "regression_performance(X_train, y_train,X_val, y_val, X_test, y_test,model)\n",
    "regression_evaluation_plots(X_train, y_train, X_val, y_val,X_test, y_test, \n",
    "                            model, alpha_scatter=0.5)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Prediction\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's take a sample from the test set and use it as if it was live data. We will consider 2 houses (not only 1)\n",
    "\n",
    "live_data = X_test[:2,:]\n",
    "live_data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We use `.predict()` and parse the data. In this case, we are predicting the price of 2 houses\n",
    "* Since the X_test data is scaled and is an array, it is difficult to make sense of the content, but we are assuming here the data passed through the pre_processing pipeline already.\n",
    "\n",
    "\n",
    "\n",
    "model.predict(live_data)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Save and Load the model\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In case you want to save your model, you may use `.save() `and parse the directory and the model's name. The extension is `.h5`\n",
    "* Remember in this notebook we used a pipeline to pre-process the data, so in a project using tabular data you would be interested to save this pipeline also as a pkl file (similarly to what we saw in the scikit-learn lesson)\n",
    "\n",
    "model.save('my_model.h5')\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> You can load the model using `load_model()` from the Keras module\n",
    "* Let's load the model as model_2\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model_2 = load_model('my_model.h5')\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> NOTE: the history information on a loaded model is lost when you save and load afterwards. The recommendation is to fit the model, generate the training history plots and save them immediately after\n",
    "\n",
    "model_2.history.history\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
