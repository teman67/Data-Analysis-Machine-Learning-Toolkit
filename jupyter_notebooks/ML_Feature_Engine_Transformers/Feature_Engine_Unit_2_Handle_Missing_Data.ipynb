{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 02 - Handle Missing Data\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn how to handle Missing Data on numerical and categorical data\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Missing Data\n",
    "\n",
    "Feature-engine imputes missing data with values learned from the data or arbitrary values set by the user, either from numerical or categorical variables. We will study:\n",
    "* Mean Median Imputer\n",
    "* Arbitrary Number Imputer\n",
    "* Categorical Imputer\n",
    "* Drop Missing Data\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Mean Median Imputer\n",
    "\n",
    "It replaces missing data with the mean or median value of the variable. It works only with numerical variables. The documentation link is [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/MeanMedianImputer.html)\n",
    "\n",
    "\n",
    "* Parse a list of variables to be imputed. Alternatively, this imputer can automatically select all variables of type numeric.\n",
    "* The imputer first calculates the mean/median values of the variables (with the fit method). Then replaces the missing data with the estimated value (with the transform method).\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "How do you know if you should impute the mean or the median?\n",
    "* You should assess the numerical distribution plot. If it is normally distributed (bell curve shape), you can replace missing values as the mean. Otherwise, replace with the median\n",
    "\n",
    "Let's load the penguins dataset\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df.sample(frac=0.5, random_state=5)\n",
    "df.head()\n",
    "\n",
    "Let's check missing data levels with `.isnull().sum()`\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "We assess the distribution. We will replace the values with the median.\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "for col in ['bill_length_mm' , 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "  sns.histplot(data=df, x=col, kde=True)\n",
    "  plt.show()\n",
    "  print('\\n')\n",
    "\n",
    "We load and set the transformer. The arguments are:\n",
    "* imputation_method: either mean or median\n",
    "* variables: list of numerical variables to apply the method to. If you dont parse anything here, the transformer will consider all numerical variables\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "imputer = MeanMedianImputer(imputation_method='median',\n",
    "                            variables=['bill_length_mm' , 'bill_depth_mm',\n",
    "                                       'flipper_length_mm', 'body_mass_g'])\n",
    "\n",
    "We use the `.fit()` method, so the transformer can learn the median values from the selected variables. The argument is the dataset you are interested to learn from\n",
    "\n",
    "imputer.fit(df)\n",
    "\n",
    "As a learning step, let's check the learned values with the attribute `.imputer_dict_`\n",
    "\n",
    "imputer.imputer_dict_\n",
    "\n",
    "we now transform the data, which means we replace the missing data of each variable according to its respective learned median value. We use `.transform()` method. The argument is the dataset you want to transform\n",
    "\n",
    "df = imputer.transform(df)\n",
    "\n",
    "Check the output, it is a DataFrame.\n",
    "* At first, we may think this is a minor detail; however other libraries for feature engineer, like scikit-learn, return as an array a ``.transform()`` command when doing a data transformation\n",
    "\n",
    "print(type(df))\n",
    "df.head()\n",
    "\n",
    "Let's check missing levels on `['bill_length_mm' , 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']`. They were replaced with median values\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Let's use an example where we arrange a transformer in a pipeline. We will use this approach from now on. \n",
    "* First we reload the dataset with missing data\n",
    "\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df.sample(frac=0.5, random_state=5)\n",
    "df.isnull().sum()\n",
    "\n",
    "We set the pipeline in 1 step. We name it as 'median'. Then we use the `MeanMedianImputer()` and the arguments we saw earlier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
    "                                     variables=['bill_length_mm' , 'bill_depth_mm',\n",
    "                                                'flipper_length_mm', 'body_mass_g']) )\n",
    "])\n",
    "pipeline\n",
    "\n",
    "We fit the pipeline. That means we will execute all the tasks in the pipeline\n",
    "* In this example, it has 1 step which learns the median value from the selected variables\n",
    "\n",
    "pipeline.fit(df)\n",
    "\n",
    "We then transform the dataset\n",
    "\n",
    "df = pipeline.transform(df)\n",
    "\n",
    "And check for missing data\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "If we want to check the learned values from the median imputed, we have to assess the step. Using bracket notation, we write the step name\n",
    "\n",
    "pipeline['median']\n",
    "\n",
    "We then use the respective attribute from the transformer, in this case, `.imputer_dict_`\n",
    "\n",
    "pipeline['median'].imputer_dict_\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Arbitrary Number\n",
    "\n",
    "It replaces missing data in numerical variables with an arbitrary number determined by the user. The function documentation is [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/ArbitraryNumberImputer.html)\n",
    "* The arguments are the variables and the number to be imputed\n",
    "\n",
    "\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "\n",
    "Let's use the penguins dataset and check missing data levels\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df.sample(frac=0.5, random_state=5)\n",
    "df.isnull().sum()\n",
    "\n",
    "We set the pipeline. Imagine you conducted the same data analysis as before and decided (with no criteria) you want to impute`-100` where `bill_length_mm`, and `-500` for the remaining numerical variables with missing data.\n",
    "* The values we chose here are arbitrary. In a project, this imputation can be related to a particular business context, for example, imagine if the variable is Age and you have the long term experience that if a row is missing for this variable, you should replace it with, say, 25.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'bill_length_mm',  ArbitraryNumberImputer(arbitrary_number=-100,\n",
    "                                                  variables=['bill_length_mm']) ),\n",
    "\n",
    "      ( 'other_variables',  ArbitraryNumberImputer(arbitrary_number=-500,\n",
    "                                                   variables=['bill_depth_mm',\n",
    "                                                              'flipper_length_mm',\n",
    "                                                              'body_mass_g']) )\n",
    "\n",
    "])\n",
    "pipeline\n",
    "\n",
    "We fit the pipeline with the df\n",
    "\n",
    "pipeline.fit(df)\n",
    "\n",
    "We then transform the dataset\n",
    "\n",
    "df = pipeline.transform(df)\n",
    "\n",
    "And check for missing data\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "If we want to check the learned values from the arbitrary imputation, we have to assess the step. Using bracket notation, we write the step name. We first check `bill_length_mm`\n",
    "\n",
    "pipeline['bill_length_mm'].imputer_dict_\n",
    "\n",
    "Then for the remaining variables\n",
    "\n",
    "pipeline['other_variables'].imputer_dict_\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Categorical Imputer\n",
    "\n",
    "It replaces missing data in categorical variables by an arbitrary value (typically with the label 'missing') or by the most frequent category. The documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/CategoricalImputer.html)\n",
    "* How do we select between the most frequent category or arbitrary value imputation?\n",
    "  * It will depend on your business context and the missing levels. If you believe there is a hidden pattern that your data is missing, in this categorical variable, you can replace it with 'missing' and may expect an algorithm will find and use that for predictions.\n",
    "  * Or maybe if the missing levels are so low, you can, in theory, replace them with the most frequent level without jeopardizing the analysis\n",
    "\n",
    "\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "##### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Replace with 'Missing'\n",
    "\n",
    "Let's use the penguins dataset and check missing data levels\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df.sample(n=50, random_state=1)\n",
    "df.isnull().sum()\n",
    "\n",
    "Let's assess `sex` frequency with .value_counts()\n",
    "\n",
    "df['sex'].value_counts()\n",
    "\n",
    "We will first replace it with a 'missing' label. We set the transformer in a pipeline by defining its name and CategoricalImputer as the function. The parameters are the imputation method, the value to be filled and the variables\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'categorical_imputer', CategoricalImputer(imputation_method='missing',\n",
    "                                                  fill_value='Missing',\n",
    "                                                  variables=['sex']) )\n",
    "])\n",
    "pipeline\n",
    "\n",
    "For learning purposes, we can use `.fit_transform()`, so we can speed up the process of fitting and transforming the data. We assign the result to df\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "We check again `sex` distribution with `.value_counts()`. Now ``missing`` is a label in this variable.\n",
    "\n",
    "\n",
    "df['sex'].value_counts()\n",
    "\n",
    "##### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Replace with most frequent\n",
    "\n",
    "We will reload the penguins dataset and use the other method: impute with the most frequent\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "We set the pipeline and `.fit_transform()`\n",
    "* CategoricalImputer now has imputation method as frequent.\n",
    "\n",
    "df['sex'].value_counts()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'categorical_imputer', CategoricalImputer(imputation_method='frequent',\n",
    "                                                  variables=['sex']) )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "We check again `sex` distribution with `.value_counts()`.\n",
    "* You may remember, at first, Male had 168 rows. Now it has increased after this transformation\n",
    "\n",
    "\n",
    "df['sex'].value_counts()\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Drop Missing Data\n",
    "\n",
    "It deletes rows with missing values, similar to `pd.drop_na()`. It can handle numerical and categorical variables.\n",
    "* the arguments are the list of variables for which missing values should be removed. When you don't set the variables list explicitly, the transformer will drop all missing data rows. The documentation link is [here](https://feature-engine.readthedocs.io/en/1.1.x/imputation/DropMissingData.html).\n",
    "* In theory, you should consider as a last resort the option to drop missing data since there was an effort and cost to collect the data. However, if you see the imputing methods will not serve you, and your missing data levels are low, you, in theory, can remove the missing data without jeopardizing the analysis\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "\n",
    "As usual, let's consider the penguins dataset and check missing data levels. We notice the dataset has 344 rows. The missing data level looks not to be significant. The majority of missing levels is 2, and there is 1 with 11\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(f\"{df.shape} \\n\")\n",
    "df.isnull().sum()\n",
    "\n",
    "We set the pipeline with this transformer - we don't parse any variables since we are interested to drop all missing data. Then we `.fit_transform()` the data\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "We check again how many rows the data has and the missing levels.\n",
    "* We notice now the data has 333 rows, before was 344 rows\n",
    "\n",
    "print(f\"{df.shape} \\n\")\n",
    "df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
