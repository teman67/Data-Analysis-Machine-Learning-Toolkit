{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engine - Unit 03 - Handle Categorical Variable Encoding\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn how to Handle Categorical Variable Encoding, using One Hot Encoder, Ordinal Encoder and Rare Label Encoder\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "And load our typical packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Handle Categorical Variable Encoding\n",
    "\n",
    "A categorical encoder replaces variable labels with a calculated or arbitrary number. We will study:\n",
    "* One Hot Encoder\n",
    "* Ordinal Encoder\n",
    "* Rare Label Encoder\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  One Hot Encoder\n",
    "\n",
    "This technique replaces the categorical variable with a combination of binary variables (which takes value 0 or 1) where each new binary variable is related to a label from the categorical variable. The function is called `OneHotEncoder()` and its documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/encoding/OneHotEncoder.html)\n",
    "* For example, imagine if our variable is `Color`, and has 3 labels: Yellow, Blue and Green\n",
    "* When you One Hot Encode (OHE) `Color`, it is replaced by 3 binary variables `Color_Yellow`, `Color_Blue` and `Color_Green`\n",
    "* Imagine if a given row of Color is Yellow. Once One Hot Encoded, this row will be transformed to  Color_Yellow = 1, Color_Blue = 0 and Color_Green = 0.\n",
    "* There is a concept called redundant feature. You may think for a moment: do I need 3 binary variables to represent the variable `Color`? \n",
    "  * The answer is no. If you have 2 binary variables for Color, say Color_Yellow and Color_Blue, you will represent all possibilities since: \n",
    "    * Color_Yellow = 1 and Color_Blue = 0, means yellow\n",
    "    * Color_Yellow = 0  and Color_Blue = 1, means blue\n",
    "    * Color_Yellow = 0  and Color_Blue = 0, means green\n",
    "\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "Let's consider only categorical variables from the penguin dataset\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url)\n",
    "# df = sns.load_dataset('penguins').filter(['species', 'island', 'sex'])\n",
    "df.head()\n",
    "\n",
    "Let's create the pipeline with 2 steps (Handle Missing data and categorical encoding), and then use `.fit_transform()`\n",
    "* Note: we can't encode a categorical variable that has missing data. For the exercise, we dropped the missing data using the transformer from the previous unit (DropMissingData)\n",
    "* Using OneHotEncoder we parse a list of variables that we are interested to OHE.\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ('drop_na', DropMissingData() ),\n",
    "      ('ohe', OneHotEncoder(variables=['species', 'island', 'sex']) )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df\n",
    "\n",
    "But what about the redundant feature?\n",
    "* You just have to parse `drop_last=True` at `OneHotEncoder()`\n",
    "* But first we reload the dataset\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url).filter(['species', 'island', 'sex'])\n",
    "# df = sns.load_dataset('penguins').filter(['species', 'island', 'sex'])\n",
    "df.head()\n",
    "\n",
    "Then set the same pipeline, but now adding `drop_last=True`. Compare to the previous transformation and check which binary variables were removed\n",
    "* Note there are only 2 binary variables related to species and island. There is only one binary variable related to sex. This same set of variables carries the same amount of information as the previous OHE transformation.\n",
    "* You probably noticed that this transformation has the potential to generate a lot of new columns. That increases the feature space and may increase the chance of overfitting your model. To manage that, you may use, when possible, a FeatureSelection() step in your pipeline to select the most relevant features in your dataset. Don't worry. This topic will be covered in the next lesson.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('ohe', OneHotEncoder(variables=['species', 'island', 'sex'], drop_last=True) )\n",
    "])\n",
    "\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Ordinal Encoder\n",
    "\n",
    "It replaces categories with ordinal numbers, like 0, 1, 2, 3 etc.  \n",
    "* The numbers can be on a first seen first basis.\n",
    "* You can parse a list of variables to encode, or it will encode all categorical variables.\n",
    "\n",
    "The function is `OrdinalEncoder()` and its documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/encoding/OrdinalEncoder.html)\n",
    "\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "Let's consider categorical variables from the penguin dataset\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(url).filter(['species', 'island', 'sex'])\n",
    "# df = sns.load_dataset('penguins').filter(['species', 'island', 'sex'])\n",
    "df.head()\n",
    "\n",
    "Let's create the pipeline with 2 steps (Handle Missing data and ordinal encoding), and then use `.fit_transform()`\n",
    "* We will not parse the variables argument to `OrdinalEncoder()`, that means we will encode all variables. We set `encoding_method='arbitrary'`\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary') )\n",
    "])\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df\n",
    "\n",
    "Let's check the frequencies and labels names.\n",
    "* We use a for loop on DataFrame columns and print the variable name + the value counts for that variable\n",
    "* Note the labels were replaced by numbers. For example Male and Female, were replaced by 0 and 1\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts()} \\n\\n\")\n",
    "\n",
    "Let's check the encoder dictionary, to see how the transformer mapped the labels to numbers.\n",
    "\n",
    "pipeline['ordinal_encoder'].encoder_dict_\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Rare Label Encoder\n",
    "\n",
    "This encoder groups infrequent categories in a new category called 'Rare' (or other defined name)\n",
    "* For example, if your variable is Fruit, and the  percentage of rows for the labels banana, grape and apple is less than < 6 %, all these labels will be replaced by 'Rare'. That helps to decrease the chance of a model to overfit.\n",
    "* The function is `RareLabelEncoder()` and its documentation is found [here](https://feature-engine.readthedocs.io/en/1.1.x/encoding/RareLabelEncoder.html). The arguments are:\n",
    "  * tol, which is the tolerance, or the minimum frequency a label should have to be considered frequent. Categories with frequencies lower than tol will be replaced as 'Rare'.\n",
    "  * n_categories: The minimum number of categories a variable should have for the encoder to find frequent labels. If the variable contains fewer categories, all of them will be considered frequent.\n",
    "  * variables: list of variables that you would like to apply this transformation on. If you don't parse anything, it will select all categorical variables\n",
    "\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "\n",
    "Let's consider a few variables from the Titanic dataset. It holds passengers records from the last Titanic's ride\n",
    "* Note we are converting the variables to 'object' with `.astype()` since some of them were listed as numerical yet being represented as a ``category``.\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url).filter(['parch', 'sibsp']).astype('object')\n",
    "# df = sns.load_dataset('titanic').filter(['parch', 'sibsp']).astype('object')\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "Let's assess missing levels\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "Now let's check the label's frequencies for each variable\n",
    "* We loop on each variable and count its labels frequencies using .value_counts(normalize=True)\n",
    "* We note that there some labels which are infrequent, like 6 for parch\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")\n",
    "\n",
    "Let's create the pipeline with 2 steps (rare label encoding), and then use `.fit_transform()`. We show here the use case where we can perform multiple rare label encoding\n",
    "* The first RareLabelEncoder deals with parch and sets the tolerance to 10% (this is a random number and is used to explain the concept). In the end, any parch label that is less frequent than 10%, will be replaced by 'Rare'\n",
    "* The second RareLabelEncoder deals with sibsp and sets the tolerance to 8% (again, random number to illustrate the concept). In the end, any sibsp label that is less frequent than 8%, will be replaced by 'Rare'\n",
    "* Note: you can perform this technique with a set of variables. We created the example with single variables with different tolerance to illustrate the concept. In the workplace, the tol level will be selected based on the business context.\n",
    "* We set ``n_categories=2`` since we want to encode all possible labels.\n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('rle_parch', RareLabelEncoder(tol=0.1,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['parch']) ), \n",
    "      ('rle_sibsp', RareLabelEncoder(tol=0.08,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['sibsp']) )\n",
    "])\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "df.head()\n",
    "\n",
    "Now let's check the label's frequencies for each variable again\n",
    "* Note the labels were grouped into a label called 'Rare' according to the rules defined in the pipeline.\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")\n",
    "\n",
    "You may think for a moment: but my variable is still a category, what should I do?\n",
    "* The answer is, arrange a Ordinal Encoder or OHE after a rare label encoder, so your categorical variables can be properly encoded\n",
    "* Just as an example, let's reload the data and inspect labels frequencies\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url).filter(['parch', 'sibsp']).astype('object')\n",
    "# df = sns.load_dataset('titanic').filter(['parch', 'sibsp']).astype('object')\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")\n",
    "\n",
    "In one cell, we will do the following tasks:\n",
    "* create a pipeline with 4 steps: drop missing data, 2 rare label encoders and ordinal encoder\n",
    "* then we fit and transform the data\n",
    "* finally, we loop over the variables to check labels frequencies \n",
    "\n",
    "from feature_engine.imputation import DropMissingData\n",
    "pipeline = Pipeline([\n",
    "      ( 'drop_na', DropMissingData() ),\n",
    "      ('rle_parch', RareLabelEncoder(tol=0.1,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['parch']) ), \n",
    "      ('rle_sibsp', RareLabelEncoder(tol=0.08,\n",
    "                                     n_categories=2,\n",
    "                                     variables=['sibsp']) ),\n",
    "      ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary',\n",
    "                                         variables= ['parch', 'sibsp']) )\n",
    "])\n",
    "\n",
    "df = pipeline.fit_transform(df)\n",
    "\n",
    "for col in df.columns.to_list():\n",
    "  print(f\"{col} \\n{df[col].value_counts(normalize=True)} \\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
