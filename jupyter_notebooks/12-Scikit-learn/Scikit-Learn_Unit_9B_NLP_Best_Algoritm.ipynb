{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 09 - NLP (Natural Language Processing)\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Understand and create a ML pipeline for NLP (Natural Language Processing)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - NLP (Natural Language Processing)\n",
    "\n",
    "* We will continue from the previous notebook, where we found the algorithm that most suited the data (SGDClassifier) and now we are doing an extensive hyperparameter optimization to find the pipeline with the best hyperparameter combination.\n",
    "* Once we find the best pipeline, we will evaluate the pipeline and predict on real-time data\n",
    "* We will need to re-load the data, custom function for hyperparameter optimization and pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Load data\n",
    "\n",
    "We will use a dataset that contains records telling if a given SMS message is a spam or not (spam or ham). We load the data from GitHub.\n",
    "* In this project we are intersted to **predict if a given message is spam or not**, therefore the ML task is Classification\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ShresthaSudip/SMS_Spam_Detection_DNN_LSTM_BiLSTM/master/SMSSpamCollection'\n",
    "df = (pd.read_csv(url, sep ='\\t',names=[\"label\", \"message\"])\n",
    "    .sample(frac=0.6, random_state=0)\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "df = df.sample(frac=0.5, random_state=101)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Split data\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\">  As usual, we are splitting the data into train and test set.\n",
    "* In this case, there are 2 columns in the dataset, where `message` contains the text, and `label` tells if the SMS message was a spam or not.\n",
    "* At the end, we have a Pandas Series for the features (`message`) and target (`label`) - note the brackets subsetting the data, for example: `df['message']`\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'],\n",
    "                                                    test_size=0.2, random_state=101)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Create the pipeline\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> We will consider classic steps in an NLP pipeline, where we first clean the text then extract the features for the model\n",
    "* The pipeline steps will be slightly different from what we have been studying at Classfication (Data Cleaning, Feature Engineering, Feature Scaling, Feature Selection and Model), but the purpose is the same: prepare the data for the model.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Tips.png\n",
    "\">\n",
    " Overall, here we will consider steps for **(1) cleaning the textual data and (2) represent the text as numbers, or feature extraction.**\n",
    "* (1) In our case, we will make the text lower case and remove punctuation for text cleaning.\n",
    "    * The practical tasks for cleaning the textual data will differ from dataset to dataset; for example, you may have a dataset where you need to clean HTML tags, so you need a function to do that for you; or eventually, you need to remove diacritics (marks located above or below a letter to reflect a particular pronunciation, like *resumé*)\n",
    "  \n",
    "* (2) There are also multiple techniques for feature extraction; we will consider the ones we covered in Module 2; in this case, we **will tokenize the text then use TF-IDF (Term Frequency－Inverse Document Frequency)**\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are using texthero to clean the textual data, **by changing the text to lowercase and removing punctuation** from the textual data\n",
    "* If you want to refresh these concepts, you may revert to Module 2 where we covered the NLP task.\n",
    "* We need to create a custom Python class to parse it into the pipeline afterwards. We are using the same approach for creating custom transformers we saw in the feature-engine lesson, where we use BaseEstimator, TransformerMixin, create fit and transform methods. So the custom transformer can be added correctly to the ML pipeline.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import texthero as hero\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class text_cleaning(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def __init__(self ):\n",
    "    return None\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    X = hero.preprocessing.lowercase(X)\n",
    "    X = hero.remove_punctuation(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> For feature extraction we use **CountVectorizer** and **TfidfTransformer**, you can find their documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).\n",
    "* We need to convert the textual data to a format that the algorithms can learn the relationships from, also known as vectors. \n",
    "  * CountVectorizer: According to its documentation, it converts a collection of text documents to a matrix of token counts. It stores the number of times every word is used in our text data. We are also removing english \"stop words\".\n",
    "  * (TfidfTransformer) Term Frequency－Inverse Document Frequency Transformer: It transforms a count matrix to a normalized tf or tf-idf representation according to its documentation. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and are empirically less informative than features that occur in a small fraction of the data. In addition, this highlights the words that are most unique to a document, thus better for characterizing it. \n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Steps.png\n",
    "\"> Finally, our pipeline will have 4 steps:\n",
    "* Text cleaning: lowercase the text and remove punctuation\n",
    "* CountVectorizer: convert text to token\n",
    "* TF-IDF: transform a count matrix to a normalized tf or tf-idf representation\n",
    "* Model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "  pipeline = Pipeline([\n",
    "                       \n",
    "        ( 'text_cleaning', text_cleaning() ),\n",
    "        ( 'vect', CountVectorizer(stop_words='english') ),\n",
    "        ( 'tfidf', TfidfTransformer() ),\n",
    "        ( 'model', model )\n",
    "    ])\n",
    "  \n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We load the Python class (HyperparameterOptimizationSearch) that we studied in previous units, which aims to fit a set of algorithms with multiple hyperparameters. A quick reinforcement on what this class does: \n",
    "* The developer defines a set of algorithms and their respectives hyperparameters values\n",
    "* The code iterates on each algoirthm and fits pipelines using GridSearchCV considering its respective hyperparameter values. The result is stored.\n",
    "That is repeated for all algorithms that the user listed.\n",
    "* Once all pipelines are trained, the developer can retrieve a list with a performance result summary and an object that contains all trained pipelines. The developer can then subset the best pipeline.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model=  PipelineOptimization(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Fit multiple pipelines for best algorithms using multiple hyperparameter combination\n",
    "\n",
    "We update our dictionaries using the algorithms and hyperparameters combinations we want to optimize.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "models_search = {\n",
    "    \"SGDClassifier\":SGDClassifier(random_state=101),}\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"SGDClassifier\": {'model__tol':[1e-2, 1e-1], },\n",
    "  }\n",
    "\n",
    "Next we fit multiple pipelines using the algorithms we selected considering multiple combinations of hyperparameters\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='accuracy',\n",
    "           n_jobs=-2,\n",
    "           cv=2)\n",
    "\n",
    "Let's check  training results summary \n",
    "* Note that SGDClassifier performed best. Not only the performance improved from the default hyperparameters but now SGDClassifier is performing better than LinearSVC.\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary\n",
    "\n",
    "We check for the best model programatically\n",
    "\n",
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model\n",
    "\n",
    "So we can grab the best model parameters\n",
    "\n",
    "grid_search_pipelines[best_model].best_params_\n",
    "\n",
    "And grab the best pipeline\n",
    "\n",
    "best_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pipeline Performance\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Finally, we evaluate the pipeline as usual with our custom function for classification tasks.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We parse the arguments we are familiar with\n",
    "* Train and Test set\n",
    "* Best pipeline\n",
    "* for `label_map`, we get the classes name with `.unique()`\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Result.png\n",
    "\"> Note: The model learned the relationships in the data in the train set and predicted everything correctly. In the test set, we had a few misclassifications, but still, the performance looks good, and the **model could generalize on the unseen data** (test set)\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=best_pipeline,\n",
    "                label_map= df['label'].unique()\n",
    "                )\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Predict on real time data\n",
    "\n",
    "Parse a real time message to validate whether or not you shall click on the link   :)\n",
    "* Try new sentencs, by changing the content on the `real_time_msg` variable\n",
    "\n",
    "########################################################################\n",
    "real_time_msg = 'Congratulations, you won the auction. Please click on link below to get your prize'\n",
    "########################################################################\n",
    "\n",
    "X_live = pd.Series(data=real_time_msg, name='message')\n",
    "best_pipeline.predict(X_live)\n",
    "\n",
    "---\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Bonus: Typical hyperparameters for algorithms listed in this notebook\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We again reinforce that It will take time and experience to learn which hyperparameters to consider when optimizing your pipeline and which values would make sense to tune.\n",
    "* the library documentation is your best friend instructing you on the library's available hyperparameters for that given algorithm.\n",
    "The hyperparameters we list here are a suggestion so that you can use them as a reference when you start fine-tuning your ML pipelines.\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> We will write the hyperparameters for the algorithms using the same dictionary structure we saw over the notebook, assuming you are arranging everything into a pipeline and the last step is called '`model`'\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Support Vector Machine\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Support Vector Machine (or SVM) is an algorithm that can be used for Classification or Regression\n",
    "* The idea is to find a hyperplane that separates the data.\n",
    "  * A hyperplane is a boundary that distinguish the data points and will be N-1 dimensiona;, for example, if you have 2 variables (2 dimensions), you can plot these variables in a XY plot, like a 2D scatter plot. Your hyperplane in this case is a line. If you have 3 variables  (3 dimensions), you can plot these variables in a XYZ plot, like a 3D scatter plot. Your hyperplane in this case is a [plane](https://en.wikipedia.org/wiki/Plane_(geometry)) (note: it is a geometry plane, not an airplane)\n",
    "  * The hyperplane should have the maximum distance (here called margin) between data points. Support vectors (therefore the algorithm name) are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params_search = {\n",
    "    \"SVC\": {#'model__C':[1,0.5,1.5],\n",
    "          'model__tol':[1e-3,1e-2,1e-4],\n",
    "          #  'model__kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "            }\n",
    "}\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Linear Support Vector Machine\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to its documentation, Linear Support Vector Machine is similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "params_search = {\n",
    "\n",
    "    \"LinearSVC\": {#'model__C':[1,0.5,1.5],\n",
    "                  'model__tol':[1e-3,1e-2,1e-4],\n",
    "                  },\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Linear classifier with SGD (Stochastic Gradient Descent)\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to Scikit learn documentation, this estimator implements regularized linear models (SVM, logistic regression, etc.) with stochastic gradient descent (SGD) learning\n",
    "* SGD is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "params_search = {\n",
    "    \"SGDClassifier\": {'model__tol':[1e-3, 1e-2, 1e-4],\n",
    "                    #  'model__penalty':['l2', 'l1', 'elasticnet'],\n",
    "                     # 'model__alpha':[0.0001,0.001],\n",
    "                      },\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Naive Bayes\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to Scikit learn [documentation](https://scikit-learn.org/stable/modules/naive_bayes.html), Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. \n",
    "* Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "params_search = {\n",
    "    \"MultinomialNB\":{'model__alpha': [1.0, 0.6, 0.4, 1.3, 0.0]\n",
    "                     },\n",
    "}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
