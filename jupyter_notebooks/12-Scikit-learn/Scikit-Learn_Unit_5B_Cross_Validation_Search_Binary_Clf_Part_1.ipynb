{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn - Unit 05 - Cross Validation Search (GridSearchCV) and Hyperparameter Optimization Binary Clf- Part 01\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives\n",
    "\n",
    "* Learn and use GridSearchCV for Hyperparameter Optimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning\n",
    "\n",
    "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "---\n",
    "\n",
    "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 05 - Cross Validation Search (GridSearchCV ) and Hyperparameter Optimization\n",
    "\n",
    "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Hyperparameter Optimization with 1 algorithm\n",
    "\n",
    "---\n",
    "\n",
    "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\">  Binary Classification\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In the last section, we saw how to conduct a hyperparameter tuning using one algorithm to solve a Regression problem.\n",
    "* There is a tiny difference for using GridSearch CV when your ML task is classification, we will cover that now.\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We are going to consider a similar workflow to the one we studied earlier:\n",
    "* Split the data\n",
    "* Define the pipeline and hyperparameter\n",
    "* Fit the pipeline\n",
    "* Evaluate the pipeline\n",
    "\n",
    "Let's load the breast cancer data from sklearn. It shows records for a breast mass sample and a diagnosis informing whether it is as malignant or benign cancer, where 0 is malignant and 1 is benign.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df_clf = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df_clf['diagnostic'] = pd.Series(data.target)\n",
    "df_clf = df_clf.sample(frac=0.5, random_state=101)\n",
    "\n",
    "\n",
    "print(df_clf.shape)\n",
    "df_clf.head()\n",
    "\n",
    "We split the data into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(\n",
    "                                    df_clf.drop(['diagnostic'],axis=1),\n",
    "                                    df_clf['diagnostic'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    "                                    )\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "And create a pipeline with 3 steps, feature scaling, feature selection and modelling using RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def pipeline_clf():\n",
    "  pipeline = Pipeline([\n",
    "      ( \"feat_scaling\",StandardScaler() ),\n",
    "      ( \"feat_selection\",SelectFromModel(RandomForestClassifier(random_state=101)) ),\n",
    "      ( \"model\", RandomForestClassifier(random_state=101)),\n",
    "\n",
    "    ])\n",
    "\n",
    "  return pipeline\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We define our hyperparameter list based on the algorithm documentation. One method could be to consider the default parameter value and a set of values that are around the default value\n",
    "* In this case, there are 2 possible combinations of hyperparameter\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"model__n_estimators\":[50,20],\n",
    "              }\n",
    "\n",
    "param_grid\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> When we move to Classification, there will be a different GridSearchCV scoring argument. \n",
    "\n",
    "\n",
    "\n",
    "* We consider that in our classification projects, the potential performance metrics are: accuracy, recall, precision, f1 score.\n",
    "  * When the metric is either recall, precision or f1 score, we need to inform which class we want to tune for and use `make_scorer()` as an \"auxiliary\" function to help with defining the metric and the class to tune. The documentation for make_scorer is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html)\n",
    "  * When your performance metric is recall, you need to import [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), if is precision, [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and if it is F1 score, you need to import [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html); so you can parse the metric to the `make_scorer()` function.\n",
    "  * When your performance metric is accuracy, you simply write \"accuracy\" for scoring: `scoring='accuracy'`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In this exercise, we have 0 and 1 as diagnostics for breast cancer. \n",
    "* We assume that when defining the ML business case, it was agreed that the performance metric is recall on malignant (0), since the client needs to detect a malignant case. \n",
    "* The client doesn't want to miss a malignant case, even if that comes with a cost where you misidentify someone that is benign, and you say it is malignant. For this client, this is not as bad a misidentifying someone as benign when malignant. Therefore the model is tuned on recall for malignant (0) \n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.metrics import f1_score # in case your metric is f1 score, you would need this import\n",
    "from sklearn.metrics import precision_score # in case your metric is precision, you would need this import\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\">  The arguments `estimator, param_grid, cv, n_jobs, verbose` are similar to the previous example.\n",
    "\n",
    "* The focus is now on `scoring` when creating the object to conduct a grid search. You will need `make_scorer()` to parse your tune on recall for class 0 for this binary classifier. \n",
    "  * Parse at `make_scorer()`, recall_score as your metric and pos_label to identify which class you want to tune recall. In this case, it is 0.\n",
    "* Next, you fit the grid-search with the train set (features and target) as usual\n",
    "\n",
    "\n",
    "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Since `cv=2`, we will fit 2 models for each hyperparameter combination using k-fold cross-validation. Therefore 4 models (2 times 2) are trained in the end. \n",
    "* The same dynamic repeats: computes the performance for each cross validated model and gets the average performance for a given hyperparemeter combination, then iterate for each hyperparamter combination \n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=pipeline_clf(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=2,\n",
    "                    n_jobs=-2,\n",
    "                    verbose=3,\n",
    "                    # in the workplace we typically set verbose to 1, \n",
    "                    # to reduce the amount of messages when fitting the models\n",
    "                    # for teaching purpose, we set to 3 to see the score for each cross validated model\n",
    "                    scoring=make_scorer(recall_score, pos_label=0)\n",
    "                    )\n",
    "\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "Next, we check the results for all 4 different models  with `.cv_results_` and use the same code from the previous section\n",
    "* Note that `'model__n_estimators': 50` gave a average recall score on class 0 of 0.86 and is superior than the other combination\n",
    "\n",
    "(pd.DataFrame(grid.cv_results_)\n",
    ".sort_values(by='mean_test_score',ascending=False)\n",
    ".filter(['params','mean_test_score'])\n",
    ".values\n",
    " )\n",
    "\n",
    "Let's check the best parameters with `.best_params_`\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "And finally grab the pipeline that has the best estimator, the one which gave the highest score. \n",
    "\n",
    "pipeline = grid.best_estimator_\n",
    "pipeline\n",
    "\n",
    "As usual in our workflow, we will evaluate the pipeline using our custom function for classification problems\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
    "\n",
    "  prediction = pipeline.predict(X)\n",
    "\n",
    "  print('---  Confusion Matrix  ---')\n",
    "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
    "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
    "        ))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "  print('---  Classification Report  ---')\n",
    "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
    "  print(\"#### Train Set #### \\n\")\n",
    "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
    "\n",
    "  print(\"#### Test Set ####\\n\")\n",
    "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)\n",
    "\n",
    "We parse the parameters as usual, considering that class 0 is malignant and class 1 is benign, therefore label_map receives a ordered list that matches the class value and its meaning:  `['malignant', 'benign']`\n",
    "* Note the recall on malignant on the train set is 100% and on the test set is 90%. In a project, you will have set the threshold you would accept. \n",
    "* In case the threshold you agreed with the client is 90%, this pipeline is the solution for your case. In case your threshold is 98%, you would still have to look for other algorithms or hyperparameters combinations to improve your pipeline performance\n",
    "\n",
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline,\n",
    "                label_map= ['malignant', 'benign'] \n",
    "                )\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
